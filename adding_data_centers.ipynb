{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import gurobipy as gp\n",
        "from gurobipy import GRB\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from scipy.interpolate import griddata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  super().__setitem__(key, value)\n",
            "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  super().__setitem__(key, value)\n",
            "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  super().__setitem__(key, value)\n"
          ]
        }
      ],
      "source": [
        "# Arizona shapefile\n",
        "\n",
        "us_states = gpd.read_file('cb_2018_us_state_500k/cb_2018_us_state_500k.shp')\n",
        "arizona = us_states[us_states['NAME'] == 'Arizona']\n",
        "arizona.to_crs(\"EPSG:5070\", inplace=True) # we use EPSG 5070 to get more accurate area measurements. https://epsg.io/5070-1252\n",
        "\n",
        "# water basins shapefile\n",
        "\n",
        "huc_subbasins = gpd.read_file('HUC8_CONUS/HUC8_US.shp') # read in subbasins\n",
        "huc_subbasins['has_arizona'] = huc_subbasins['STATES'].map(lambda x: 'AZ' in x)\n",
        "huc_subbasins_az = huc_subbasins[huc_subbasins['has_arizona']]\n",
        "huc_subbasins_az.to_crs(\"EPSG:5070\", inplace=True)\n",
        "\n",
        "huc_subbasins_az['HUC6'] = huc_subbasins_az['HUC8'].map(lambda x: x[:6])\n",
        "\n",
        "# also, in case we want centroid:\n",
        "# huc_subbasins_az['centroid'] = huc_subbasins_az.to_crs(\"+proj=cea\").centroid.to_crs(huc_subbasins_az.crs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get lists of HUC8 subbasins that group into HUC6 basins\n",
        "huc6_huc8_dict = dict()\n",
        "\n",
        "for huc6 in huc_subbasins_az['HUC6'].unique():\n",
        "    huc_filter = huc_subbasins_az[huc_subbasins_az['HUC6'] == huc6]\n",
        "    huc6_huc8_dict[huc6] = huc_filter['HUC8'].to_list()\n",
        "\n",
        "# data values for HUC8 regions generated from the paper by Abu Bakar Siddik et al 2021: https://iopscience.iop.org/article/10.1088/1748-9326/abfba1?_sp=b48260d8-0a7b-4784-9d4b-0e1ac60ee727\n",
        "water_carbon = pd.read_excel(\"14504913/SI_XLS/Results.xlsx\", sheet_name=\"Table 3\", skiprows=1)\n",
        "\n",
        "water_carbon['HUC8_str'] = water_carbon['HUC8'].map(lambda x: ''.join(['0']*(8-len(str(x)))) + str(x))\n",
        "\n",
        "# filter to only HUC rows\n",
        "water_carbon['not_nan'] = ~water_carbon['WSF_1MW_DC'].isna()\n",
        "water_carbon = water_carbon[water_carbon['not_nan']]\n",
        "\n",
        "# now merge with the arizona HUC data\n",
        "huc_subbasins_az_join = huc_subbasins_az.merge(water_carbon, how=\"inner\", left_on=\"HUC8\", right_on=\"HUC8_str\")\n",
        "huc_subbasins_az_join = gpd.clip(huc_subbasins_az_join, arizona)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# take a look at water stress factor, carbon factor data\n",
        "wsf_dict = {k: v for k, v in zip(huc_subbasins_az_join['HUC8_y'], huc_subbasins_az_join['WSF_1MW_DC'])}\n",
        "cf_dict = {k: v for k, v in zip(huc_subbasins_az_join['HUC8_y'], huc_subbasins_az_join['CF_1MW_DC'])}\n",
        "\n",
        "s_max = huc_subbasins_az_join['WSF_1MW_DC'].max()\n",
        "e_max = huc_subbasins_az_join['CF_1MW_DC'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# estimations of existing data centers: https://www.datacentermap.com/\n",
        "# power of existing data centers? Unclear. https://baxtel.com/data-center/phoenix\n",
        "# well, in total there is around 602.8 MW in the Phoenix area. https://azbigmedia.com/real-estate/phoenix-ranks-4th-among-north-american-data-center-markets/\n",
        "# and, in the Phoenix area there are 124 data centers. https://www.datacentermap.com/usa/arizona/\n",
        "# so, let's say on average, each existing data center takes on 5 MW of load.\n",
        "\n",
        "# existing inventory HUC8\n",
        "# 15050301: 6\n",
        "# 15050302: 2\n",
        "# 15050303: 4 (from Eloy) \n",
        "# 15050100: 17 (from Mesa airport AZA) 13 (from Chandler airport CHD) 4 (from Gilbert)\n",
        "# 15060106: 2 (from Mesa) 25 (from Phoenix airport and Scottsdale) 2 (from upper Scottsdale)\n",
        "# 15070102: 6 (from Deer Valley) 40 (from Litchfield park airport)\n",
        "# 15070103: 1 (from Buckeye/Arlington)\n",
        "\n",
        "\n",
        "# totals (done by hand, because data lies behind paywalls):\n",
        "# 15050301: 6\n",
        "# 15050302: 2\n",
        "# 15050303: 4\n",
        "# 15050100: 34\n",
        "# 15060106: 29\n",
        "# 15070102: 46\n",
        "# 15070103: 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  super().__setitem__(key, value)\n"
          ]
        }
      ],
      "source": [
        "# electricity prices\n",
        "# https://www.energysage.com/local-data/electricity-cost/az/\n",
        "# cochise county: 16 cents/kwh\n",
        "# coconino county: 15 cents/kwh\n",
        "# gila county: 20 cents/kwh\n",
        "# la paz county: 15 cents/kwh\n",
        "# maricopa county: 15 cents/kwh\n",
        "# mohave county: 18 cents/kwh\n",
        "# navajo county: 15 cents/kwh\n",
        "# pima county: 18 cents/kwh\n",
        "# pinal county: 16 cents/kwh\n",
        "# yavapai county: 15 cents/kwh\n",
        "# yuma county: 15 cents/kwh\n",
        "\n",
        "us_counties = gpd.read_file('cb_2018_us_county_500k/cb_2018_us_county_500k.shp') # read in counties\n",
        "arizona_counties = us_counties[us_counties['STATEFP'] == '04'] # filter to arizona\n",
        "arizona_counties.to_crs(\"EPSG:5070\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  super().__setitem__(key, value)\n"
          ]
        }
      ],
      "source": [
        "# units: cents/kwh\n",
        "elec_price_dict = {'Cochise': 16, 'Coconino': 15, 'Gila': 20, 'La Paz': 15, 'Maricopa': 15, 'Mohave': 18, \n",
        "                   'Navajo': 15, 'Pima': 18, 'Pinal': 16, 'Yavapai': 15, 'Yuma': 15}\n",
        "\n",
        "# new units: dollars/mw = cents/kwh * (0.01 dollar/1 cent) * (1000 kwh/1 mw)\n",
        "elec_price_dict = {k: 10*v for k, v in elec_price_dict.items()}\n",
        "\n",
        "has_elec_price = set(elec_price_dict.keys())\n",
        "num_elec_price = len(has_elec_price)\n",
        "\n",
        "# find arizona counties with electricity price data\n",
        "arizona_counties['has_elec_price'] = arizona_counties['NAME'].map(lambda x: x in has_elec_price)\n",
        "arizona_counties_price = arizona_counties[arizona_counties['has_elec_price']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# assign electricity price in HUC8 regions to closest county with price\n",
        "# consider: if desired, we can add some random noise\n",
        "\n",
        "# build up the elec_price ($/MW) column\n",
        "huc8_codes = set(huc_subbasins_az_join['HUC8_y'])\n",
        "\n",
        "for code in huc8_codes:\n",
        "    huc8_row = huc_subbasins_az_join[huc_subbasins_az_join['HUC8_y'] == code]\n",
        "    county_intersect = arizona_counties_price['geometry'].intersection(huc8_row['geometry'].iloc[0])\n",
        "    \n",
        "    area_sum = np.sum(county_intersect.area)\n",
        "\n",
        "    if area_sum > 0: # take a weighted sum based on area of intersection with counties\n",
        "        weighted_price = 0\n",
        "        for idx, val in county_intersect.area.items(): \n",
        "            weighted_price += elec_price_dict[arizona_counties_price.loc[idx, 'NAME']]*val\n",
        "        weighted_price /= area_sum\n",
        "    else: # just find the closest one\n",
        "        county_dist = arizona_counties_price['geometry'].distance(huc8_row['geometry'].iloc[0])\n",
        "        county_dist.sort_values(ascending=True, inplace=True)\n",
        "        weighted_price = elec_price_dict[arizona_counties_price.loc[county_dist.index[0], 'NAME']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "x5h-jYTzfz_g",
        "outputId": "37a189ed-20d4-4eaf-ca5e-449062417254"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 129 baseline data centers\n",
            "Created 480 potential locations\n",
            "Starting optimization with 129 data centers and 480 potential locations\n",
            "Restricted license - for non-production use only - expires 2026-11-23\n",
            "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (linux64 - \"Ubuntu 22.04.4 LTS\")\n",
            "\n",
            "CPU model: Intel(R) Xeon(R) CPU @ 2.20GHz, instruction set [SSE2|AVX|AVX2]\n",
            "Thread count: 1 physical cores, 2 logical processors, using up to 2 threads\n",
            "\n"
          ]
        },
        {
          "ename": "GurobiError",
          "evalue": "Model too large for size-limited license; visit https://gurobi.com/unrestricted for more information",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mGurobiError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-fa8e945795f4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0mmin_distance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m  \u001b[0;31m# km\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize_data_center_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_centers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpotential_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_distance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-fa8e945795f4>\u001b[0m in \u001b[0;36moptimize_data_center_locations\u001b[0;34m(baseline_centers, potential_locations, min_distance)\u001b[0m\n\u001b[1;32m    345\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddConstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"min_dist_{d1}_{l1}_{d2}_{l2}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mGRB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMAL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32msrc/gurobipy/_model.pyx\u001b[0m in \u001b[0;36mgurobipy._model.Model.optimize\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mGurobiError\u001b[0m: Model too large for size-limited license; visit https://gurobi.com/unrestricted for more information"
          ]
        }
      ],
      "source": [
        "# major cities for baseline data centers\n",
        "# basically, atm all of the data centers are in these three, so im approximating by just using their central lat/lon\n",
        "# count is how the 129 are split among the locations\n",
        "CITIES = {\n",
        "    \"Phoenix\": {\"lat\": 33.4484, \"lng\": -112.0740, \"count\": 121},\n",
        "    \"Tucson\": {\"lat\": 32.2226, \"lng\": -110.9747, \"count\": 7},\n",
        "    \"Nogales\": {\"lat\": 31.3405, \"lng\": -110.9420, \"count\": 1}\n",
        "}\n",
        "\n",
        "# create the baseline data centers locations\n",
        "def create_baseline_centers():\n",
        "    centers = []\n",
        "    center_id = 0\n",
        "\n",
        "    for city, info in CITIES.items():\n",
        "        # decided to add a slight random variation to prevent all centers being at exactly the same point\n",
        "        for i in range(info[\"count\"]):\n",
        "            #within ~5km of the city center\n",
        "            lat_variation = (np.random.random() - 0.5) * 0.05\n",
        "            lng_variation = (np.random.random() - 0.5) * 0.05\n",
        "\n",
        "            centers.append({\n",
        "                \"id\": center_id,\n",
        "                \"city\": city,\n",
        "                \"lat\": info[\"lat\"] + lat_variation,\n",
        "                \"lng\": info[\"lng\"] + lng_variation\n",
        "            })\n",
        "            center_id += 1\n",
        "\n",
        "    return centers\n",
        "\n",
        "# here, i made a grid of potential locations across the approximate Arizona\n",
        "def create_location_grid(resolution=0.2):\n",
        "    locations = []\n",
        "    loc_id = 0\n",
        "    for lat in np.arange(AZ_MIN_LAT, AZ_MAX_LAT + resolution, resolution):\n",
        "        for lng in np.arange(AZ_MIN_LNG, AZ_MAX_LNG + resolution, lng_spacing): # should this be resolution rather than lng_spacing? - RICHARD\n",
        "            locations.append({\n",
        "                \"id\": loc_id,\n",
        "                \"lat\": lat,\n",
        "                \"lng\": lng\n",
        "            })\n",
        "            loc_id += 1\n",
        "\n",
        "    return locations\n",
        "\n",
        "# generate water stress data based on the Aqueduct data approximately\n",
        "# will have to request the actual shapefiles for later on\n",
        "# so this is just simplified, with higher values = more stress (0-5 scale)\n",
        "def generate_water_stress(locations):\n",
        "\n",
        "    # northern Arizona: lower stress (more precipitation)\n",
        "    # central & Phoenix metro: high stress\n",
        "    # southern Arizona: very high stress\n",
        "    # western Arizona along colorado river: medium stress\n",
        "\n",
        "    for loc in locations:\n",
        "        lat, lng = loc[\"lat\"], loc[\"lng\"]\n",
        "        base_stress = 5.0 - (lat - AZ_MIN_LAT) / (AZ_MAX_LAT - AZ_MIN_LAT) * 2.5\n",
        "\n",
        "        # phoenix metro area: very high stress\n",
        "        if (33.0 <= lat <= 34.0) and (-112.5 <= lng <= -111.5):\n",
        "            stress = 4.5 + np.random.random() * 0.5  # 4.5-5.0\n",
        "\n",
        "        # tucson area: very high stress\n",
        "        elif (31.9 <= lat <= 32.5) and (-111.2 <= lng <= -110.7):\n",
        "            stress = 4.2 + np.random.random() * 0.8  # 4.2-5.0\n",
        "\n",
        "        # colorado river region: medium stress due to water access\n",
        "        elif lng < -113.5:\n",
        "            stress = 2.5 + np.random.random() * 1.5  # 2.5-4.0\n",
        "\n",
        "        # northern mountains (e.g., flagstaff): lower stress\n",
        "        elif (34.5 <= lat <= 36.0) and (-112.5 <= lng <= -111.0):\n",
        "            stress = 1.8 + np.random.random() * 1.2  # 1.8-3.0\n",
        "\n",
        "        #  northeastern AZ (like the navajo area): medium-high stress\n",
        "        elif lat > 35.0 and lng > -110.5:\n",
        "            stress = 3.2 + np.random.random() * 1.0  # 3.2-4.2\n",
        "\n",
        "        # if not near, we can just use base stress with some noise\n",
        "        else:\n",
        "            stress = base_stress + (np.random.random() - 0.5) * 1.0\n",
        "\n",
        "        stress = max(0, min(5, stress))\n",
        "        loc[\"water_stress\"] = stress\n",
        "\n",
        "    return locations\n",
        "\n",
        "#electricity price data by county\n",
        "\n",
        "# out of curiosity, where are the electricity prices sourced from? In case we want to reproduce\n",
        "# these electricity price calculations for other states, and I can help - RICHARD\n",
        "def generate_electricity_prices(locations):\n",
        "\n",
        "    # note: the average commercial electricity price in Arizona is ~$0.0811/kWh (8.11 cents)\n",
        "\n",
        "    for loc in locations:\n",
        "        lat, lng = loc[\"lat\"], loc[\"lng\"]\n",
        "        base_price = 0.0811\n",
        "\n",
        "        # phoenix metro area (APS & SRP territory)\n",
        "        if (33.0 <= lat <= 34.0) and (-112.5 <= lng <= -111.5):\n",
        "            if lng < -112.0:  # SRP territory\n",
        "                price = base_price * (0.92 + np.random.random() * 0.08)  # 8% lower on average\n",
        "            else:  # APS territory\n",
        "                price = base_price * (0.98 + np.random.random() * 0.07)  # 2% lower on average\n",
        "\n",
        "        # tucson area (tucson electric power)\n",
        "        elif (31.9 <= lat <= 32.5) and (-111.2 <= lng <= -110.7):\n",
        "            price = base_price * (1.03 + np.random.random() * 0.07)  # 3-10% higher\n",
        "\n",
        "        # rural areas (higher distribution costs)\n",
        "        elif lat > 35.0 or lat < 31.7 or lng < -113.0:\n",
        "            price = base_price * (1.10 + np.random.random() * 0.15)  # 10-25% higher\n",
        "\n",
        "        # otherwise, i use base price with some noise\n",
        "        else:\n",
        "            price = base_price * (0.95 + np.random.random() * 0.15)  # ±10% variation\n",
        "\n",
        "        loc[\"electricity_price\"] = price\n",
        "\n",
        "    return locations\n",
        "\n",
        "# carbon emission rate data in tons Co2/'Mwh\n",
        "def generate_carbon_emissions(locations):\n",
        "\n",
        "    # avg emissions for arizona's grid: ~0.4 tons CO2/MWh (400 kg/MWh)\n",
        "    # from what i looked up, Palo Verde Nuclear Plant creates the biggest variation\n",
        "    # there's probably a better way to do this, but for the approximation, I just use the plant to vary this val plus some of the predominant plant type variation otherwise\n",
        "\n",
        "    for loc in locations:\n",
        "        lat, lng = loc[\"lat\"], loc[\"lng\"]\n",
        "\n",
        "        base_emissions = 0.40  # tons CO2 per MWh\n",
        "\n",
        "        # areas near Palo Verde Nuclear Generating Station (west of phoenix)\n",
        "        if (33.2 <= lat <= 33.5) and (-113.0 <= lng <= -112.5):\n",
        "            emissions = base_emissions * (0.65 + np.random.random() * 0.10)  # 25-35% lower\n",
        "\n",
        "        # phoenix metro area (mixed generation)\n",
        "        elif (33.0 <= lat <= 34.0) and (-112.5 <= lng <= -111.5):\n",
        "            emissions = base_emissions * (0.90 + np.random.random() * 0.15)  # 0-25% lower\n",
        "\n",
        "        # northeastern AZ (coal plants)\n",
        "        elif lat > 35.0 and lng > -110.5:\n",
        "            emissions = base_emissions * (1.20 + np.random.random() * 0.15)  # 20-35% higher\n",
        "\n",
        "        # southern AZ (more solar penetration)\n",
        "        elif lat < 32.5:\n",
        "            emissions = base_emissions * (0.85 + np.random.random() * 0.15)  # 0-30% lower\n",
        "\n",
        "        else:\n",
        "            emissions = base_emissions * (0.90 + np.random.random() * 0.20)  # ±15% variation\n",
        "\n",
        "        loc[\"carbon_emissions\"] = emissions\n",
        "\n",
        "    return locations\n",
        "\n",
        "# water-use efficiency scores\n",
        "def generate_water_efficiency(locations):\n",
        "    # based on climate factors like temperature, humidity, and technology potential\n",
        "\n",
        "    for loc in locations:\n",
        "        lat, lng = loc[\"lat\"], loc[\"lng\"]\n",
        "\n",
        "        # it seems that higher latitudes and elevations typically have better efficiency due to cooler climate, so i just came up with this\n",
        "        base_efficiency = 3.0 + (lat - AZ_MIN_LAT) / (AZ_MAX_LAT - AZ_MIN_LAT) * 2.0\n",
        "\n",
        "        # hot desert areas (e.g., phoenix, yuma) will have poor efficiency\n",
        "        if ((33.0 <= lat <= 34.0) and (-112.5 <= lng <= -111.5)) or (lng < -113.5 and lat < 33.0):\n",
        "            efficiency = 1.5 + np.random.random() * 1.0  # 1.5-2.5 (poor)\n",
        "\n",
        "        #\\higher elevation areas ( e.g., flagstaff, high country) - good efficiency\n",
        "        elif ((35.0 <= lat <= 36.0) and (-112.5 <= lng <= -111.0)):\n",
        "            efficiency = 4.0 + np.random.random() * 1.0  # 4.0-5.0 (excellent)\n",
        "\n",
        "        # mountain transition zones will have moderate to good efficiency\n",
        "        elif ((34.0 <= lat <= 35.0) and (-112.5 <= lng <= -111.0)):\n",
        "            efficiency = 3.0 + np.random.random() * 1.5  # 3.0-4.5 (good)\n",
        "\n",
        "        else:\n",
        "            efficiency = base_efficiency + (np.random.random() - 0.5) * 1.0\n",
        "        efficiency = max(1, min(5, efficiency))\n",
        "        loc[\"water_efficiency\"] = efficiency\n",
        "\n",
        "    return locations\n",
        "\n",
        "# distance function to calculate distances between locations (in km)\n",
        "# lowkey just took this from my old UROP code, idrk if its correct lol\n",
        "def calc_distance(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth radius in kilometers\n",
        "\n",
        "    # Convert latitude and longitude from degrees to radians\n",
        "    lat1_rad = np.radians(lat1)\n",
        "    lon1_rad = np.radians(lon1)\n",
        "    lat2_rad = np.radians(lat2)\n",
        "    lon2_rad = np.radians(lon2)\n",
        "\n",
        "    # Difference in coordinates\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "\n",
        "    a = np.sin(dlat/2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon/2)**2\n",
        "    c = 2 * np.arcsin(np.sqrt(a))\n",
        "    distance = R * c\n",
        "\n",
        "    return distance\n",
        "\n",
        "# REPLACEMENT - RICHARD, taken from here: https://gis.stackexchange.com/questions/425452/calculate-distance-between-two-lat-lon-alt-points-in-python\n",
        "# from pyproj import Geod\n",
        "# g = Geod(ellps=\"WGS84\")\n",
        "# def calc_distance_alt(lat1, lon1, lat2, lon2):\n",
        "#   _, _, distance = g.inv(lon1, lat1, lon2, lat2)\n",
        "#   return distance\n",
        "\n",
        "# check if a lcation is within arizona's boundaries\n",
        "# this is simplified, w/ real implementation i think we should use a proper shape file\n",
        "# no problem! I should be able to do that in a standard manner, with geopandas. Example: https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoSeries.contains.html#geopandas.GeoSeries.contains - RICHARD\n",
        "def is_in_arizona(lat, lng):\n",
        "  # basically i made it a rectangle\n",
        "    return (AZ_MIN_LAT <= lat <= AZ_MAX_LAT) and (AZ_MIN_LNG <= lng <= AZ_MAX_LNG)\n",
        "\n",
        "# optimization function\n",
        "def optimize_data_center_locations(baseline_centers, potential_locations, min_distance=10):\n",
        "    print(f\"Starting optimization with {len(baseline_centers)} data centers and {len(potential_locations)} potential locations\")\n",
        "    np.random.seed(42)\n",
        "\n",
        "    locations = generate_water_stress(potential_locations)\n",
        "    locations = generate_electricity_prices(locations)\n",
        "    locations = generate_carbon_emissions(locations)\n",
        "    locations = generate_water_efficiency(locations)\n",
        "\n",
        "    location_ids = [loc[\"id\"] for loc in locations]\n",
        "    water_stress = {loc[\"id\"]: loc[\"water_stress\"] for loc in locations}\n",
        "    electricity_price = {loc[\"id\"]: loc[\"electricity_price\"] for loc in locations}\n",
        "    carbon_emissions = {loc[\"id\"]: loc[\"carbon_emissions\"] for loc in locations}\n",
        "    water_efficiency = {loc[\"id\"]: loc[\"water_efficiency\"] for loc in locations}\n",
        "\n",
        "    # coords for distance calculations\n",
        "    location_coords = {loc[\"id\"]: (loc[\"lat\"], loc[\"lng\"]) for loc in locations}\n",
        "\n",
        "    # normalization values for the obj function\n",
        "    max_water_stress = max(water_stress.values())\n",
        "    max_electricity_price = max(electricity_price.values())\n",
        "    max_carbon_emissions = max(carbon_emissions.values())\n",
        "    min_water_efficiency = min(water_efficiency.values())\n",
        "    max_water_efficiency = max(water_efficiency.values())\n",
        "\n",
        "    # objective weights, which can be adjusted based on priorities (kind of chose randomly, so if you want to update at any point, feel free :) )\n",
        "    # sounds great! - RICHARD\n",
        "    # my initial reaction is to set water efficiency weight to zero lol.  but i definitely have some biases/leanings, so we can definitely discuss this. - RICHARD\n",
        "    # another interesting question, kind of taking from my UROP last year, is: at what threshold of objective weights, e.g. on water, do we need before it becomes - RICHARD\n",
        "    # unreasonable to place data centers in hot, desert/dry environments? - RICHARD\n",
        "    # I suppose we can try a few different weightings. (water stress, elec, carbon, water efficiency) = (1, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0), (0, 0, 0, 1), (0.35, 0.25, 0.25, 0.15)\n",
        "    weights = {\n",
        "        \"water_stress\": 0.35,\n",
        "        \"electricity_price\": 0.25,\n",
        "        \"carbon_emissions\": 0.25,\n",
        "        \"water_efficiency\": 0.15\n",
        "    }\n",
        "\n",
        "    model = gp.Model(\"DataCenterOptimization\")\n",
        "\n",
        "    # decision variables: x[d,l] = 1 if data center d is placed at location l\n",
        "    x = {}\n",
        "    for d in range(len(baseline_centers)):\n",
        "        for l in location_ids:\n",
        "            x[d, l] = model.addVar(vtype=GRB.BINARY, name=f\"x_{d}_{l}\")\n",
        "\n",
        "    # minimize weighted combination of metrics\n",
        "    obj = gp.LinExpr()\n",
        "\n",
        "    for d in range(len(baseline_centers)):\n",
        "        for l in location_ids:\n",
        "            # note: for water efficiency, higher is better, so we invert it\n",
        "            # out of curiosity, why not 1 - water_efficiency[l]/max_water_efficiency? But works either way! - RICHARD\n",
        "            normalized_water_efficiency = 1 - (water_efficiency[l] - min_water_efficiency) / (max_water_efficiency - min_water_efficiency)\n",
        "\n",
        "            norm_water_stress = water_stress[l] / max_water_stress\n",
        "            norm_electricity = electricity_price[l] / max_electricity_price\n",
        "            norm_emissions = carbon_emissions[l] / max_carbon_emissions\n",
        "\n",
        "\n",
        "            # I believe that relocating a data center costs around $120,000: https://www.google.com/url?q=https://apposite-tech.com/best-practices-data-center-relocation/&sa=D&source=docs&ust=1744170960046669&usg=AOvVaw1nspqunGp5e_Gn6C7xgrJB. - RICHARD\n",
        "            # Perhaps we could also induce a cost for relocating a data center. So you are more reluctant to actually move a data center unless absolutely necessary. - RICHARD\n",
        "\n",
        "            weighted_sum = (\n",
        "                weights[\"water_stress\"] * norm_water_stress +\n",
        "                weights[\"electricity_price\"] * norm_electricity +\n",
        "                weights[\"carbon_emissions\"] * norm_emissions +\n",
        "                weights[\"water_efficiency\"] * normalized_water_efficiency\n",
        "            )\n",
        "\n",
        "            obj += x[d, l] * weighted_sum\n",
        "\n",
        "    model.setObjective(obj, GRB.MINIMIZE)\n",
        "\n",
        "    # each data center must be placed at exactly one location\n",
        "    for d in range(len(baseline_centers)):\n",
        "        model.addConstr(gp.quicksum(x[d, l] for l in location_ids) == 1, f\"one_loc_per_dc_{d}\")\n",
        "\n",
        "    #each location can have at most one data center\n",
        "    for l in location_ids:\n",
        "        model.addConstr(gp.quicksum(x[d, l] for d in range(len(baseline_centers))) <= 1, f\"one_dc_per_loc_{l}\")\n",
        "\n",
        "    # idk if we need this, but its basically setting a minimum distance between data centers since i dont want the model spitting them out on top of eachy other\n",
        "    # Ooo great point. I feel like this gets at our conversation about why you would ever consider clustering data centers. - RICHARD\n",
        "    # However, I wonder if the water scarcity aspect will naturally cause the model to not cram all the data centers together, if they\n",
        "    # start to pile on too much burden on the local water system. - RICHARD\n",
        "    # Anyhow, I believe the fact that you set the locations in a grid pattern at least gives some minimum spacing. We can keep talking\n",
        "    # about this more if needed! - RICHARD\n",
        "    if min_distance > 0:\n",
        "        location_distances = {}\n",
        "        for l1 in location_ids:\n",
        "            for l2 in location_ids:\n",
        "                if l1 < l2:\n",
        "                    lat1, lng1 = location_coords[l1]\n",
        "                    lat2, lng2 = location_coords[l2]\n",
        "                    dist = calc_distance(lat1, lng1, lat2, lng2)\n",
        "                    location_distances[(l1, l2)] = dist\n",
        "                    location_distances[(l2, l1)] = dist\n",
        "\n",
        "        # so basically, for locations that are too close in lat/lon, prevent placing data centers at both\n",
        "        # I wonder if the below step is contributing to the computational bottleneck. But we don't have to worry about it if it runs\n",
        "        # well on your computer locally :) - RICHARD\n",
        "        for (l1, l2), dist in location_distances.items():\n",
        "            if dist < min_distance:\n",
        "                for d1 in range(len(baseline_centers)):\n",
        "                    for d2 in range(d1 + 1, len(baseline_centers)):\n",
        "                        model.addConstr(x[d1, l1] + x[d2, l2] <= 1, f\"min_dist_{d1}_{l1}_{d2}_{l2}\")\n",
        "\n",
        "    model.optimize()\n",
        "\n",
        "    if model.status == GRB.OPTIMAL:\n",
        "        print(f\"Optimal solution found w/ objective value: {model.objVal}\")\n",
        "\n",
        "        # extract the solution\n",
        "        solution = []\n",
        "        for d in range(len(baseline_centers)):\n",
        "            dc = baseline_centers[d]\n",
        "            for l in location_ids:\n",
        "                if x[d, l].x > 0.5:\n",
        "                    loc_info = next(loc for loc in locations if loc[\"id\"] == l)\n",
        "                    solution.append({\n",
        "                        \"data_center_id\": d,\n",
        "                        \"original_city\": dc[\"city\"],\n",
        "                        \"original_lat\": dc[\"lat\"],\n",
        "                        \"original_lng\": dc[\"lng\"],\n",
        "                        \"new_lat\": loc_info[\"lat\"],\n",
        "                        \"new_lng\": loc_info[\"lng\"],\n",
        "                        \"water_stress\": loc_info[\"water_stress\"],\n",
        "                        \"electricity_price\": loc_info[\"electricity_price\"],\n",
        "                        \"carbon_emissions\": loc_info[\"carbon_emissions\"],\n",
        "                        \"water_efficiency\": loc_info[\"water_efficiency\"],\n",
        "                        \"distance\": calc_distance(\n",
        "                            dc[\"lat\"], dc[\"lng\"],\n",
        "                            loc_info[\"lat\"], loc_info[\"lng\"]\n",
        "                        )\n",
        "                        # feel free to adjujst this list, i was just making sure things seemed okay\n",
        "                    })\n",
        "                    break\n",
        "\n",
        "\n",
        "        # LEFT OFF HERE - RICHARD\n",
        "\n",
        "        baseline_metrics = []\n",
        "        for dc in baseline_centers:\n",
        "            min_dist = float('inf')\n",
        "            closest_loc = None\n",
        "\n",
        "            for loc in locations:\n",
        "                dist = calc_distance(dc[\"lat\"], dc[\"lng\"], loc[\"lat\"], loc[\"lng\"])\n",
        "                if dist < min_dist:\n",
        "                    min_dist = dist\n",
        "                    closest_loc = loc\n",
        "\n",
        "            baseline_metrics.append({\n",
        "                \"city\": dc[\"city\"],\n",
        "                \"lat\": dc[\"lat\"],\n",
        "                \"lng\": dc[\"lng\"],\n",
        "                \"water_stress\": closest_loc[\"water_stress\"],\n",
        "                \"electricity_price\": closest_loc[\"electricity_price\"],\n",
        "                \"carbon_emissions\": closest_loc[\"carbon_emissions\"],\n",
        "                \"water_efficiency\": closest_loc[\"water_efficiency\"]\n",
        "            })\n",
        "\n",
        "        # just to see the average metrics\n",
        "        baseline_avg = {\n",
        "            \"water_stress\": sum(b[\"water_stress\"] for b in baseline_metrics) / len(baseline_metrics),\n",
        "            \"electricity_price\": sum(b[\"electricity_price\"] for b in baseline_metrics) / len(baseline_metrics),\n",
        "            \"carbon_emissions\": sum(b[\"carbon_emissions\"] for b in baseline_metrics) / len(baseline_metrics),\n",
        "            \"water_efficiency\": sum(b[\"water_efficiency\"] for b in baseline_metrics) / len(baseline_metrics)\n",
        "        }\n",
        "\n",
        "        optimized_avg = {\n",
        "            \"water_stress\": sum(s[\"water_stress\"] for s in solution) / len(solution),\n",
        "            \"electricity_price\": sum(s[\"electricity_price\"] for s in solution) / len(solution),\n",
        "            \"carbon_emissions\": sum(s[\"carbon_emissions\"] for s in solution) / len(solution),\n",
        "            \"water_efficiency\": sum(s[\"water_efficiency\"] for s in solution) / len(solution),\n",
        "            \"avg_distance\": sum(s[\"distance\"] for s in solution) / len(solution)\n",
        "        }\n",
        "\n",
        "        # here are the metrics compared to the original ben chmark\n",
        "        print(\"\\nMetrics Comparison:\")\n",
        "        print(f\"Water Stress: {baseline_avg['water_stress']:.2f} → {optimized_avg['water_stress']:.2f} \" +\n",
        "              f\"({((baseline_avg['water_stress'] - optimized_avg['water_stress']) / baseline_avg['water_stress'] * 100):.2f}% improvement)\")\n",
        "\n",
        "        print(f\"Electricity Price: ${baseline_avg['electricity_price']:.4f}/kWh → ${optimized_avg['electricity_price']:.4f}/kWh \" +\n",
        "              f\"({((baseline_avg['electricity_price'] - optimized_avg['electricity_price']) / baseline_avg['electricity_price'] * 100):.2f}% improvement)\")\n",
        "\n",
        "        print(f\"Carbon Emissions: {baseline_avg['carbon_emissions']:.4f} tons CO2/MWh → {optimized_avg['carbon_emissions']:.4f} tons CO2/MWh \" +\n",
        "              f\"({((baseline_avg['carbon_emissions'] - optimized_avg['carbon_emissions']) / baseline_avg['carbon_emissions'] * 100):.2f}% improvement)\")\n",
        "\n",
        "        print(f\"Water Efficiency: {baseline_avg['water_efficiency']:.2f} → {optimized_avg['water_efficiency']:.2f} \" +\n",
        "              f\"({((optimized_avg['water_efficiency'] - baseline_avg['water_efficiency']) / baseline_avg['water_efficiency'] * 100):.2f}% improvement)\")\n",
        "\n",
        "        print(f\"Average Relocation Distance: {optimized_avg['avg_distance']:.2f} km\")\n",
        "\n",
        "        return {\n",
        "            \"objective_value\": model.objVal,\n",
        "            \"solution\": solution,\n",
        "            \"baseline\": baseline_metrics,\n",
        "            \"metrics\": {\n",
        "                \"baseline\": baseline_avg,\n",
        "                \"optimized\": optimized_avg\n",
        "            }\n",
        "        }\n",
        "    else:\n",
        "        print(f\"No optimal solution found. Status: {model.status}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "# \\main execution\n",
        "if __name__ == \"__main__\":\n",
        "  #grid resoljution\n",
        "    resolution = 0.25  # 0.25 degrees = ~28 km\n",
        "\n",
        "    # adjust the spacing based on longitude (since longitude degrees are smaller at higher latitudes)\n",
        "    # again, this is from my old UROP, so could be slightly off\n",
        "    avg_lat = (AZ_MIN_LAT + AZ_MAX_LAT) / 2\n",
        "    lng_spacing = resolution / np.cos(np.radians(avg_lat))\n",
        "\n",
        "    # create baseline data centers (129 total for az)\n",
        "    baseline_centers = create_baseline_centers()\n",
        "    print(f\"Created {len(baseline_centers)} baseline data centers\")\n",
        "\n",
        "    # make a grid of potential locations\n",
        "    potential_locations = create_location_grid(resolution)\n",
        "    print(f\"Created {len(potential_locations)} potential locations\")\n",
        "\n",
        "    # minimum distance between data centers (in km)\n",
        "    # just set it to 0 to disable this constraint\n",
        "    min_distance = 20  # km\n",
        "\n",
        "    results = optimize_data_center_locations(baseline_centers, potential_locations, min_distance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HxFfgSQHAYo",
        "outputId": "b2d76246-a160-4b31-8c6b-88cdf33ef4ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gurobipy\n",
            "  Downloading gurobipy-12.0.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (16 kB)\n",
            "Downloading gurobipy-12.0.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (14.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.4/14.4 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gurobipy\n",
            "Successfully installed gurobipy-12.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gurobipy"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
