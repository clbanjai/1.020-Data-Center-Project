{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import gurobipy as gp\n",
        "from gurobipy import GRB\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from scipy.interpolate import griddata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  super().__setitem__(key, value)\n",
            "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  super().__setitem__(key, value)\n",
            "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  super().__setitem__(key, value)\n"
          ]
        }
      ],
      "source": [
        "# Arizona shapefile\n",
        "\n",
        "us_states = gpd.read_file('/Users/richy/Downloads/Classes/Spring_2025/1.020/Project/cb_2018_us_state_500k/cb_2018_us_state_500k.shp')\n",
        "arizona = us_states[us_states['NAME'] == 'Arizona']\n",
        "arizona.to_crs(\"EPSG:5070\", inplace=True) # we use EPSG 5070 to get more accurate area measurements. https://epsg.io/5070-1252\n",
        "\n",
        "# water basins shapefile\n",
        "\n",
        "huc_subbasins = gpd.read_file('/Users/richy/Downloads/Classes/Spring_2025/1.020/Project/HUC8_CONUS/HUC8_US.shp') # read in subbasins\n",
        "huc_subbasins['has_arizona'] = huc_subbasins['STATES'].map(lambda x: 'AZ' in x)\n",
        "huc_subbasins_az = huc_subbasins[huc_subbasins['has_arizona']]\n",
        "huc_subbasins_az.to_crs(\"EPSG:5070\", inplace=True)\n",
        "\n",
        "huc_subbasins_az['HUC6'] = huc_subbasins_az['HUC8'].map(lambda x: x[:6])\n",
        "\n",
        "# also, in case we want centroid:\n",
        "# huc_subbasins_az['centroid'] = huc_subbasins_az.to_crs(\"+proj=cea\").centroid.to_crs(huc_subbasins_az.crs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get lists of HUC8 subbasins that group into HUC6 basins\n",
        "huc6_huc8_dict = dict()\n",
        "\n",
        "for huc6 in huc_subbasins_az['HUC6'].unique():\n",
        "    huc_filter = huc_subbasins_az[huc_subbasins_az['HUC6'] == huc6]\n",
        "    huc6_huc8_dict[huc6] = huc_filter['HUC8'].to_list()\n",
        "\n",
        "# data values for HUC8 regions generated from the paper by Abu Bakar Siddik et al 2021: https://iopscience.iop.org/article/10.1088/1748-9326/abfba1?_sp=b48260d8-0a7b-4784-9d4b-0e1ac60ee727\n",
        "water_carbon = pd.read_excel(\"/Users/richy/Downloads/Classes/Spring_2025/1.020/Project/DC_footprint/SI_XLS/Results.xlsx\", sheet_name=\"Table 3\", skiprows=1)\n",
        "\n",
        "water_carbon['HUC8_str'] = water_carbon['HUC8'].map(lambda x: ''.join(['0']*(8-len(str(x)))) + str(x))\n",
        "\n",
        "# filter to only HUC rows\n",
        "water_carbon['not_nan'] = ~water_carbon['WSF_1MW_DC'].isna()\n",
        "water_carbon = water_carbon[water_carbon['not_nan']]\n",
        "\n",
        "# now merge with the arizona HUC data\n",
        "huc_subbasins_az_join = huc_subbasins_az.merge(water_carbon, how=\"inner\", left_on=\"HUC8\", right_on=\"HUC8_str\")\n",
        "huc_subbasins_az_join = gpd.clip(huc_subbasins_az_join, arizona)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# estimations of existing data centers: https://www.datacentermap.com/\n",
        "# power of existing data centers? Unclear. https://baxtel.com/data-center/phoenix\n",
        "# well, in total there is around 602.8 MW in the Phoenix area. https://azbigmedia.com/real-estate/phoenix-ranks-4th-among-north-american-data-center-markets/\n",
        "# and, in the Phoenix area there are 124 data centers. https://www.datacentermap.com/usa/arizona/\n",
        "# so, let's say on average, each existing data center takes on 5 MW of load.\n",
        "\n",
        "# existing inventory HUC8\n",
        "# 15050301: 6\n",
        "# 15050302: 2\n",
        "# 15050303: 4 (from Eloy) \n",
        "# 15050100: 17 (from Mesa airport AZA) 13 (from Chandler airport CHD) 4 (from Gilbert)\n",
        "# 15060106: 2 (from Mesa) 25 (from Phoenix airport and Scottsdale) 2 (from upper Scottsdale)\n",
        "# 15070102: 6 (from Deer Valley) 40 (from Litchfield park airport)\n",
        "# 15070103: 1 (from Buckeye/Arlington)\n",
        "\n",
        "\n",
        "# totals (done by hand, because data lies behind paywalls):\n",
        "# 15050301: 6\n",
        "# 15050302: 2\n",
        "# 15050303: 4\n",
        "# 15050100: 34\n",
        "# 15060106: 29\n",
        "# 15070102: 46\n",
        "# 15070103: 1\n",
        "\n",
        "existing_data_centers = {\n",
        "    15050301: 6,\n",
        "    15050302: 2,\n",
        "    15050303: 4,\n",
        "    15050100: 34,\n",
        "    15060106: 29,\n",
        "    15070102: 46,\n",
        "    15070103: 1\n",
        "}\n",
        "\n",
        "huc_subbasins_az_join['existing_MW'] = huc_subbasins_az_join['HUC8_y'].map(lambda x: existing_data_centers.get(x, 0)*5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  super().__setitem__(key, value)\n"
          ]
        }
      ],
      "source": [
        "# read in US counties\n",
        "us_counties = gpd.read_file('/Users/richy/Downloads/Classes/Spring_2025/1.020/Project/cb_2018_us_county_500k/cb_2018_us_county_500k.shp') # read in counties\n",
        "arizona_counties = us_counties[us_counties['STATEFP'] == '04'] # filter to arizona\n",
        "arizona_counties.to_crs(\"EPSG:5070\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  super().__setitem__(key, value)\n",
            "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  super().__setitem__(key, value)\n"
          ]
        }
      ],
      "source": [
        "# electricity prices\n",
        "# https://www.energysage.com/local-data/electricity-cost/az/\n",
        "# cochise county: 16 cents/kwh\n",
        "# coconino county: 15 cents/kwh\n",
        "# gila county: 20 cents/kwh\n",
        "# la paz county: 15 cents/kwh\n",
        "# maricopa county: 15 cents/kwh\n",
        "# mohave county: 18 cents/kwh\n",
        "# navajo county: 15 cents/kwh\n",
        "# pima county: 18 cents/kwh\n",
        "# pinal county: 16 cents/kwh\n",
        "# yavapai county: 15 cents/kwh\n",
        "# yuma county: 15 cents/kwh\n",
        "\n",
        "# units: cents/kwh\n",
        "elec_price_dict = {'Cochise': 16, 'Coconino': 15, 'Gila': 20, 'La Paz': 15, 'Maricopa': 15, 'Mohave': 18, \n",
        "                   'Navajo': 15, 'Pima': 18, 'Pinal': 16, 'Yavapai': 15, 'Yuma': 15}\n",
        "\n",
        "# new units: dollars/mw = cents/kwh * (0.01 dollar/1 cent) * (1000 kwh/1 mw)\n",
        "elec_price_dict = {k: 10*v for k, v in elec_price_dict.items()}\n",
        "\n",
        "has_elec_price = set(elec_price_dict.keys())\n",
        "num_elec_price = len(has_elec_price)\n",
        "\n",
        "# find arizona counties with electricity price data\n",
        "arizona_counties['has_elec_price'] = arizona_counties['NAME'].map(lambda x: x in has_elec_price)\n",
        "arizona_counties_elec_price = arizona_counties[arizona_counties['has_elec_price']]\n",
        "arizona_counties_elec_price['elec_price'] = arizona_counties_elec_price['NAME'].map(lambda x: elec_price_dict[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO move to a Python utils file\n",
        "\n",
        "class resolve_regions:\n",
        "    \"\"\" \n",
        "    Converts one subdivision of space (e.g., county) into another subdivision of space (e.g., HUC8.)\n",
        "\n",
        "    Averages out value of interest according to amount of overlap with the first subdivision of space, or the \n",
        "    closest region. \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, from_df, to_df, value_col, id_row):\n",
        "        \"\"\" \n",
        "        Init method for the class resolve_regions.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            from_df: gpd.GeoDataFrame\n",
        "                Dataframe to convert from\n",
        "            to_df: gpd.GeoDataFrame\n",
        "                Dataframe to convert to\n",
        "            value_col: string\n",
        "                Name of column with values of interest\n",
        "            id_row: string\n",
        "                Name of column with unique identifiers, in to_df\n",
        "        \"\"\"\n",
        "\n",
        "        self.from_df = from_df\n",
        "        self.to_df = to_df\n",
        "        self.value_col = value_col\n",
        "        self.id_row = id_row\n",
        "\n",
        "# TODO: currently this only handles average. We can also functionalize it to handle other functions.\n",
        "    def weighted_computation(self, id):\n",
        "        \"\"\" \n",
        "        Perform the weighted average computation for the output row with identification id.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            id: string\n",
        "                Unique ID for the output row\n",
        "        \"\"\"\n",
        "\n",
        "        to_geometry = self.to_df[self.to_df[self.id_row] == id]['geometry'].iloc[0] # extract geometry of to_df\n",
        "        from_to_intersect = self.from_df['geometry'].intersection(to_geometry) # obtain intersections\n",
        "\n",
        "        area_sum = np.sum(from_to_intersect.area)\n",
        "\n",
        "        if area_sum > 0: # some intersection\n",
        "            weighted_val = 0 # keep track of weighted sum for value of interest\n",
        "\n",
        "            for idx, area in from_to_intersect.area.items():\n",
        "                weighted_val += self.from_df.loc[idx, self.value_col]*area\n",
        "            \n",
        "            weighted_val /= area_sum\n",
        "        \n",
        "        else:\n",
        "            from_to_distance = self.from_df['geometry'].distance(to_geometry) # obtain distances\n",
        "            from_to_distance.sort_values(ascending=True, inplace=True)\n",
        "            lowest_idx = from_to_distance.index[0]\n",
        "\n",
        "            weighted_val = self.from_df.loc[lowest_idx, self.value_col]\n",
        "\n",
        "        return weighted_val\n",
        "\n",
        "    def convert_regions(self):\n",
        "        \"\"\" \n",
        "        Perform conversion between the subdivisions of space, and average the value of interest.\n",
        "        \"\"\"\n",
        "\n",
        "        converted_value_list = []\n",
        "\n",
        "        for id in self.to_df[self.id_row]: # iterate over geometries of to_df\n",
        "            converted_value_list.append(self.weighted_computation(id))\n",
        "\n",
        "        self.to_df[self.value_col] = converted_value_list # set the output values after conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# assign electricity price in HUC8 regions to closest county with price\n",
        "# consider: if desired, we can add some random noise\n",
        "\n",
        "# build up the elec_price ($/MW) column\n",
        "\n",
        "county_to_huc_elec_price = resolve_regions(arizona_counties_elec_price, huc_subbasins_az_join, 'elec_price', 'HUC8_y')\n",
        "county_to_huc_elec_price.convert_regions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# next, let's take a look at land prices. \n",
        "# we get TIF files from here: https://www.pnas.org/doi/10.1073/pnas.2012865117. Units: log(2017$/hectare)\n",
        "# first, let's convert into linear units.\n",
        "\n",
        "from osgeo import gdal\n",
        "\n",
        "import rasterio\n",
        "\n",
        "# perhaps we can use this for modifying the tif file: https://github.com/rasterio/rasterio/discussions/3006\n",
        "\n",
        "if False: # only need to do this once\n",
        "    # convert $/hectare to $/acre\n",
        "    acre_to_hectare = 2.47105\n",
        "\n",
        "    # original data is in ln(2017$/hectare). Transform to linear scaling\n",
        "    land_vacant = rasterio.open('/Users/richy/Downloads/Classes/Spring_2025/1.020/Project/doi_10_5061_dryad_np5hqbzq9__v20201008/places_fmv_pnas_dryad/1 estimates/places_fmv_vacant.tif')\n",
        "    land_data_np = land_vacant.read(1)\n",
        "    land_vacant.close()\n",
        "\n",
        "    land_data_np = np.where(land_data_np != 0, np.exp(land_data_np), 0) # transform the data to 2017$/hectare\n",
        "    land_data_np = land_data_np/acre_to_hectare # transform the data to 2017$/acre\n",
        "\n",
        "    # save back to tif\n",
        "    with rasterio.open('/Users/richy/Downloads/Classes/Spring_2025/1.020/Project/doi_10_5061_dryad_np5hqbzq9__v20201008/places_fmv_pnas_dryad/1 estimates/land_cost_linear.tif', 'r+') as f:\n",
        "        f.write(land_data_np, indexes=1)\n",
        "\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# now, we resample the raster first to avoid high compute times: Raster > Projections > Warp (Reproject). We resample using median, so that we can account for/ignore outliers: really expensive pieces of land (ex. Manhattan).\n",
        "# we then convert raster to polygon using QGIS. Raster > Conversion > Polygonize (Raster to Vector)\n",
        "\n",
        "# import the converted polygon file\n",
        "land_costs_shp = gpd.read_file('/Users/richy/Downloads/Classes/Spring_2025/1.020/Project/doi_10_5061_dryad_np5hqbzq9__v20201008/places_fmv_pnas_dryad/1 estimates/land_cost_linear_10000x10000.shp')\n",
        "land_costs_shp.to_crs('EPSG:5070', inplace=True)\n",
        "land_costs_shp_az = land_costs_shp[land_costs_shp.intersects(arizona.iloc[0,-1])]\n",
        "\n",
        "# NOTE: to increase accuracy, make sure to account for public land and Native American land. https://www.pnas.org/doi/10.1073/pnas.2012865117"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute estimated land costs in each HUC8\n",
        "# takes around 40 seconds to run. if desired, can even upscale the resolution further (to 20000x20000?)\n",
        "pixel_to_huc_land_cost = resolve_regions(land_costs_shp, huc_subbasins_az_join, 'DN', 'HUC8_y')\n",
        "pixel_to_huc_land_cost.convert_regions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# requirement for acres of land: https://www.jll.com/en-us/insights/how-to-assess-a-propertys-data-center-potential#:~:text=Data%20centers%20built%20for%20AI,multiple%20power%20substations%20on%20site.\n",
        "# lets say 1 GW = 1000MW requires 500 acres. So each MW requires 0.5 acres.\n",
        "\n",
        "# let's say data centers have a lifespan of 20 years: https://www.datacenterdynamics.com/en/analysis/the-data-center-life-story/.\n",
        "\n",
        "# time to convert everything to the right units...\n",
        "\n",
        "# WSF_1MW_DC: (m^3-eq water/MWh) * (24h/day) * (365.25day/year) * 20 year * 1MW = 24 * 365.25 * 20 multiplying factor\n",
        "# CF_1MW_DC: (kg-eq CO2/MWh) * (24h/day) * (365.25day/year) * 20 year * 1MW = 24 * 365.25 * 20 multiplying factor\n",
        "# elec_price: ($/MWh) * (24h/day) * (365.25day/year) * 20 year * 1MW = 24 * 365.25 * 20 multiplying factor\n",
        "# land price: ($/acre) * (0.5acre/MW) * 1 MW = 0.5 multiplying factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "hours_multiplying = 24 * 365.25 * 20 # hours/day * days/year * years lifespan\n",
        "land_multiplying = 0.5 # 0.5 acre/MW\n",
        "\n",
        "huc_subbasins_az_join['WSF_1MW_DC'] = huc_subbasins_az_join['WSF_1MW_DC'] * hours_multiplying\n",
        "huc_subbasins_az_join['CF_1MW_DC'] = huc_subbasins_az_join['CF_1MW_DC'] * hours_multiplying\n",
        "huc_subbasins_az_join['elec_price'] = huc_subbasins_az_join['elec_price'] * hours_multiplying\n",
        "huc_subbasins_az_join['DN'] = huc_subbasins_az_join['DN'] * land_multiplying\n",
        "\n",
        "huc_subbasins_az_join['tot_price'] = huc_subbasins_az_join['elec_price'] + huc_subbasins_az_join['DN']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# take a look at water stress factor, carbon factor data\n",
        "# wsf_dict = {k: v for k, v in zip(huc_subbasins_az_join['HUC8_y'], huc_subbasins_az_join['WSF_1MW_DC'])}\n",
        "# cf_dict = {k: v for k, v in zip(huc_subbasins_az_join['HUC8_y'], huc_subbasins_az_join['CF_1MW_DC'])}\n",
        "\n",
        "s_max = huc_subbasins_az_join['WSF_1MW_DC'].max()\n",
        "eplp_max = huc_subbasins_az_join['tot_price'].max()\n",
        "e_max = huc_subbasins_az_join['CF_1MW_DC'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (mac64[arm] - Darwin 24.4.0 24E263)\n",
            "\n",
            "CPU model: Apple M1 Pro\n",
            "Thread count: 10 physical cores, 10 logical processors, using up to 10 threads\n",
            "\n",
            "Optimize a model with 19 rows, 85 columns and 186 nonzeros\n",
            "Model fingerprint: 0x64daa3a9\n",
            "Coefficient statistics:\n",
            "  Matrix range     [1e+00, 5e+07]\n",
            "  Objective range  [2e-08, 8e-01]\n",
            "  Bounds range     [0e+00, 0e+00]\n",
            "  RHS range        [1e+02, 5e+09]\n",
            "Warning: Model contains large rhs\n",
            "         Consider reformulating model or setting NumericFocus parameter\n",
            "         to avoid numerical issues.\n",
            "Presolve removed 0 rows and 6 columns\n",
            "Presolve time: 0.01s\n",
            "Presolved: 19 rows, 79 columns, 174 nonzeros\n",
            "\n",
            "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
            "       0    0.0000000e+00   2.098861e+05   0.000000e+00      0s\n",
            "       3    1.2961251e+02   0.000000e+00   0.000000e+00      0s\n",
            "\n",
            "Solved in 3 iterations and 0.01 seconds (0.00 work units)\n",
            "Optimal objective  1.296125053e+02\n"
          ]
        }
      ],
      "source": [
        "# huc8 list\n",
        "huc8_iter = huc_subbasins_az_join['HUC8_y']\n",
        "\n",
        "# set index for dataframe\n",
        "huc_subbasins_az_join.set_index(huc8_iter, inplace=True)\n",
        "\n",
        "# implement optimization model\n",
        "\n",
        "# number of MW required to add\n",
        "add_req = 100\n",
        "\n",
        "# weights for the objective function\n",
        "weights = {\n",
        "    'WSF_1MW_DC': 0.4,\n",
        "    'tot_price': 0.3,\n",
        "    'CF_1MW_DC': 0.3\n",
        "}\n",
        "\n",
        "# normalization, maximum values\n",
        "max_vals = {\n",
        "    'WSF_1MW_DC': s_max,\n",
        "    'tot_price': eplp_max,\n",
        "    'CF_1MW_DC': e_max\n",
        "}\n",
        "\n",
        "model = gp.Model(\"AddDataCenters\")\n",
        "\n",
        "# decision variables: amount of MW to put in each HUC8 subbasin\n",
        "x = dict()\n",
        "\n",
        "for huc8_code in huc8_iter:\n",
        "    x[huc8_code] = model.addVar(lb=0, name=f\"x_{huc8_code}\")\n",
        "\n",
        "# another decision variable: maximum water stress in any subbasin\n",
        "basin_max = model.addVar(lb=0, name=\"basin_max\")\n",
        "\n",
        "# develop objective\n",
        "obj = gp.LinExpr()\n",
        "\n",
        "for huc8_code in huc8_iter:\n",
        "    for key in weights.keys():\n",
        "        obj += x[huc8_code]*(weights[key]*(huc_subbasins_az_join.loc[huc8_code, key]/max_vals[key]))\n",
        "\n",
        "obj += basin_max/s_max\n",
        "\n",
        "model.setObjective(obj, GRB.MINIMIZE)\n",
        "\n",
        "# add in the constraints\n",
        "\n",
        "# basin_max is at least the water stress in any subbasin\n",
        "for huc6_code, huc8_components in huc6_huc8_dict.items():\n",
        "    # consider existing data centers\n",
        "    model.addConstr(basin_max >= gp.quicksum((x[int(huc8_code)]+huc_subbasins_az_join.loc[int(huc8_code), 'existing_MW'])*huc_subbasins_az_join.loc[int(huc8_code), 'WSF_1MW_DC'] for huc8_code in huc8_components), name=f\"basin_max_{huc6_code}\")\n",
        "\n",
        "# add sufficient capacity\n",
        "model.addConstr(gp.quicksum(x[huc8_code] for huc8_code in huc8_iter) >= add_req, name=\"add_req\")\n",
        "\n",
        "# optimize!\n",
        "model.optimize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 100.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 5239855252.963221]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.x # ahh it's all getting placed in one location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "x5h-jYTzfz_g",
        "outputId": "37a189ed-20d4-4eaf-ca5e-449062417254"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 129 baseline data centers\n",
            "Created 480 potential locations\n",
            "Starting optimization with 129 data centers and 480 potential locations\n",
            "Restricted license - for non-production use only - expires 2026-11-23\n",
            "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (linux64 - \"Ubuntu 22.04.4 LTS\")\n",
            "\n",
            "CPU model: Intel(R) Xeon(R) CPU @ 2.20GHz, instruction set [SSE2|AVX|AVX2]\n",
            "Thread count: 1 physical cores, 2 logical processors, using up to 2 threads\n",
            "\n"
          ]
        },
        {
          "ename": "GurobiError",
          "evalue": "Model too large for size-limited license; visit https://gurobi.com/unrestricted for more information",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mGurobiError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-fa8e945795f4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0mmin_distance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m  \u001b[0;31m# km\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize_data_center_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_centers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpotential_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_distance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-fa8e945795f4>\u001b[0m in \u001b[0;36moptimize_data_center_locations\u001b[0;34m(baseline_centers, potential_locations, min_distance)\u001b[0m\n\u001b[1;32m    345\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddConstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"min_dist_{d1}_{l1}_{d2}_{l2}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mGRB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMAL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32msrc/gurobipy/_model.pyx\u001b[0m in \u001b[0;36mgurobipy._model.Model.optimize\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mGurobiError\u001b[0m: Model too large for size-limited license; visit https://gurobi.com/unrestricted for more information"
          ]
        }
      ],
      "source": [
        "# major cities for baseline data centers\n",
        "# basically, atm all of the data centers are in these three, so im approximating by just using their central lat/lon\n",
        "# count is how the 129 are split among the locations\n",
        "CITIES = {\n",
        "    \"Phoenix\": {\"lat\": 33.4484, \"lng\": -112.0740, \"count\": 121},\n",
        "    \"Tucson\": {\"lat\": 32.2226, \"lng\": -110.9747, \"count\": 7},\n",
        "    \"Nogales\": {\"lat\": 31.3405, \"lng\": -110.9420, \"count\": 1}\n",
        "}\n",
        "\n",
        "# create the baseline data centers locations\n",
        "def create_baseline_centers():\n",
        "    centers = []\n",
        "    center_id = 0\n",
        "\n",
        "    for city, info in CITIES.items():\n",
        "        # decided to add a slight random variation to prevent all centers being at exactly the same point\n",
        "        for i in range(info[\"count\"]):\n",
        "            #within ~5km of the city center\n",
        "            lat_variation = (np.random.random() - 0.5) * 0.05\n",
        "            lng_variation = (np.random.random() - 0.5) * 0.05\n",
        "\n",
        "            centers.append({\n",
        "                \"id\": center_id,\n",
        "                \"city\": city,\n",
        "                \"lat\": info[\"lat\"] + lat_variation,\n",
        "                \"lng\": info[\"lng\"] + lng_variation\n",
        "            })\n",
        "            center_id += 1\n",
        "\n",
        "    return centers\n",
        "\n",
        "# here, i made a grid of potential locations across the approximate Arizona\n",
        "def create_location_grid(resolution=0.2):\n",
        "    locations = []\n",
        "    loc_id = 0\n",
        "    for lat in np.arange(AZ_MIN_LAT, AZ_MAX_LAT + resolution, resolution):\n",
        "        for lng in np.arange(AZ_MIN_LNG, AZ_MAX_LNG + resolution, lng_spacing): # should this be resolution rather than lng_spacing? - RICHARD\n",
        "            locations.append({\n",
        "                \"id\": loc_id,\n",
        "                \"lat\": lat,\n",
        "                \"lng\": lng\n",
        "            })\n",
        "            loc_id += 1\n",
        "\n",
        "    return locations\n",
        "\n",
        "# generate water stress data based on the Aqueduct data approximately\n",
        "# will have to request the actual shapefiles for later on\n",
        "# so this is just simplified, with higher values = more stress (0-5 scale)\n",
        "def generate_water_stress(locations):\n",
        "\n",
        "    # northern Arizona: lower stress (more precipitation)\n",
        "    # central & Phoenix metro: high stress\n",
        "    # southern Arizona: very high stress\n",
        "    # western Arizona along colorado river: medium stress\n",
        "\n",
        "    for loc in locations:\n",
        "        lat, lng = loc[\"lat\"], loc[\"lng\"]\n",
        "        base_stress = 5.0 - (lat - AZ_MIN_LAT) / (AZ_MAX_LAT - AZ_MIN_LAT) * 2.5\n",
        "\n",
        "        # phoenix metro area: very high stress\n",
        "        if (33.0 <= lat <= 34.0) and (-112.5 <= lng <= -111.5):\n",
        "            stress = 4.5 + np.random.random() * 0.5  # 4.5-5.0\n",
        "\n",
        "        # tucson area: very high stress\n",
        "        elif (31.9 <= lat <= 32.5) and (-111.2 <= lng <= -110.7):\n",
        "            stress = 4.2 + np.random.random() * 0.8  # 4.2-5.0\n",
        "\n",
        "        # colorado river region: medium stress due to water access\n",
        "        elif lng < -113.5:\n",
        "            stress = 2.5 + np.random.random() * 1.5  # 2.5-4.0\n",
        "\n",
        "        # northern mountains (e.g., flagstaff): lower stress\n",
        "        elif (34.5 <= lat <= 36.0) and (-112.5 <= lng <= -111.0):\n",
        "            stress = 1.8 + np.random.random() * 1.2  # 1.8-3.0\n",
        "\n",
        "        #  northeastern AZ (like the navajo area): medium-high stress\n",
        "        elif lat > 35.0 and lng > -110.5:\n",
        "            stress = 3.2 + np.random.random() * 1.0  # 3.2-4.2\n",
        "\n",
        "        # if not near, we can just use base stress with some noise\n",
        "        else:\n",
        "            stress = base_stress + (np.random.random() - 0.5) * 1.0\n",
        "\n",
        "        stress = max(0, min(5, stress))\n",
        "        loc[\"water_stress\"] = stress\n",
        "\n",
        "    return locations\n",
        "\n",
        "#electricity price data by county\n",
        "\n",
        "# out of curiosity, where are the electricity prices sourced from? In case we want to reproduce\n",
        "# these electricity price calculations for other states, and I can help - RICHARD\n",
        "def generate_electricity_prices(locations):\n",
        "\n",
        "    # note: the average commercial electricity price in Arizona is ~$0.0811/kWh (8.11 cents)\n",
        "\n",
        "    for loc in locations:\n",
        "        lat, lng = loc[\"lat\"], loc[\"lng\"]\n",
        "        base_price = 0.0811\n",
        "\n",
        "        # phoenix metro area (APS & SRP territory)\n",
        "        if (33.0 <= lat <= 34.0) and (-112.5 <= lng <= -111.5):\n",
        "            if lng < -112.0:  # SRP territory\n",
        "                price = base_price * (0.92 + np.random.random() * 0.08)  # 8% lower on average\n",
        "            else:  # APS territory\n",
        "                price = base_price * (0.98 + np.random.random() * 0.07)  # 2% lower on average\n",
        "\n",
        "        # tucson area (tucson electric power)\n",
        "        elif (31.9 <= lat <= 32.5) and (-111.2 <= lng <= -110.7):\n",
        "            price = base_price * (1.03 + np.random.random() * 0.07)  # 3-10% higher\n",
        "\n",
        "        # rural areas (higher distribution costs)\n",
        "        elif lat > 35.0 or lat < 31.7 or lng < -113.0:\n",
        "            price = base_price * (1.10 + np.random.random() * 0.15)  # 10-25% higher\n",
        "\n",
        "        # otherwise, i use base price with some noise\n",
        "        else:\n",
        "            price = base_price * (0.95 + np.random.random() * 0.15)  # ±10% variation\n",
        "\n",
        "        loc[\"electricity_price\"] = price\n",
        "\n",
        "    return locations\n",
        "\n",
        "# carbon emission rate data in tons Co2/'Mwh\n",
        "def generate_carbon_emissions(locations):\n",
        "\n",
        "    # avg emissions for arizona's grid: ~0.4 tons CO2/MWh (400 kg/MWh)\n",
        "    # from what i looked up, Palo Verde Nuclear Plant creates the biggest variation\n",
        "    # there's probably a better way to do this, but for the approximation, I just use the plant to vary this val plus some of the predominant plant type variation otherwise\n",
        "\n",
        "    for loc in locations:\n",
        "        lat, lng = loc[\"lat\"], loc[\"lng\"]\n",
        "\n",
        "        base_emissions = 0.40  # tons CO2 per MWh\n",
        "\n",
        "        # areas near Palo Verde Nuclear Generating Station (west of phoenix)\n",
        "        if (33.2 <= lat <= 33.5) and (-113.0 <= lng <= -112.5):\n",
        "            emissions = base_emissions * (0.65 + np.random.random() * 0.10)  # 25-35% lower\n",
        "\n",
        "        # phoenix metro area (mixed generation)\n",
        "        elif (33.0 <= lat <= 34.0) and (-112.5 <= lng <= -111.5):\n",
        "            emissions = base_emissions * (0.90 + np.random.random() * 0.15)  # 0-25% lower\n",
        "\n",
        "        # northeastern AZ (coal plants)\n",
        "        elif lat > 35.0 and lng > -110.5:\n",
        "            emissions = base_emissions * (1.20 + np.random.random() * 0.15)  # 20-35% higher\n",
        "\n",
        "        # southern AZ (more solar penetration)\n",
        "        elif lat < 32.5:\n",
        "            emissions = base_emissions * (0.85 + np.random.random() * 0.15)  # 0-30% lower\n",
        "\n",
        "        else:\n",
        "            emissions = base_emissions * (0.90 + np.random.random() * 0.20)  # ±15% variation\n",
        "\n",
        "        loc[\"carbon_emissions\"] = emissions\n",
        "\n",
        "    return locations\n",
        "\n",
        "# water-use efficiency scores\n",
        "def generate_water_efficiency(locations):\n",
        "    # based on climate factors like temperature, humidity, and technology potential\n",
        "\n",
        "    for loc in locations:\n",
        "        lat, lng = loc[\"lat\"], loc[\"lng\"]\n",
        "\n",
        "        # it seems that higher latitudes and elevations typically have better efficiency due to cooler climate, so i just came up with this\n",
        "        base_efficiency = 3.0 + (lat - AZ_MIN_LAT) / (AZ_MAX_LAT - AZ_MIN_LAT) * 2.0\n",
        "\n",
        "        # hot desert areas (e.g., phoenix, yuma) will have poor efficiency\n",
        "        if ((33.0 <= lat <= 34.0) and (-112.5 <= lng <= -111.5)) or (lng < -113.5 and lat < 33.0):\n",
        "            efficiency = 1.5 + np.random.random() * 1.0  # 1.5-2.5 (poor)\n",
        "\n",
        "        #\\higher elevation areas ( e.g., flagstaff, high country) - good efficiency\n",
        "        elif ((35.0 <= lat <= 36.0) and (-112.5 <= lng <= -111.0)):\n",
        "            efficiency = 4.0 + np.random.random() * 1.0  # 4.0-5.0 (excellent)\n",
        "\n",
        "        # mountain transition zones will have moderate to good efficiency\n",
        "        elif ((34.0 <= lat <= 35.0) and (-112.5 <= lng <= -111.0)):\n",
        "            efficiency = 3.0 + np.random.random() * 1.5  # 3.0-4.5 (good)\n",
        "\n",
        "        else:\n",
        "            efficiency = base_efficiency + (np.random.random() - 0.5) * 1.0\n",
        "        efficiency = max(1, min(5, efficiency))\n",
        "        loc[\"water_efficiency\"] = efficiency\n",
        "\n",
        "    return locations\n",
        "\n",
        "# distance function to calculate distances between locations (in km)\n",
        "# lowkey just took this from my old UROP code, idrk if its correct lol\n",
        "def calc_distance(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth radius in kilometers\n",
        "\n",
        "    # Convert latitude and longitude from degrees to radians\n",
        "    lat1_rad = np.radians(lat1)\n",
        "    lon1_rad = np.radians(lon1)\n",
        "    lat2_rad = np.radians(lat2)\n",
        "    lon2_rad = np.radians(lon2)\n",
        "\n",
        "    # Difference in coordinates\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "\n",
        "    a = np.sin(dlat/2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon/2)**2\n",
        "    c = 2 * np.arcsin(np.sqrt(a))\n",
        "    distance = R * c\n",
        "\n",
        "    return distance\n",
        "\n",
        "# REPLACEMENT - RICHARD, taken from here: https://gis.stackexchange.com/questions/425452/calculate-distance-between-two-lat-lon-alt-points-in-python\n",
        "# from pyproj import Geod\n",
        "# g = Geod(ellps=\"WGS84\")\n",
        "# def calc_distance_alt(lat1, lon1, lat2, lon2):\n",
        "#   _, _, distance = g.inv(lon1, lat1, lon2, lat2)\n",
        "#   return distance\n",
        "\n",
        "# check if a lcation is within arizona's boundaries\n",
        "# this is simplified, w/ real implementation i think we should use a proper shape file\n",
        "# no problem! I should be able to do that in a standard manner, with geopandas. Example: https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoSeries.contains.html#geopandas.GeoSeries.contains - RICHARD\n",
        "def is_in_arizona(lat, lng):\n",
        "  # basically i made it a rectangle\n",
        "    return (AZ_MIN_LAT <= lat <= AZ_MAX_LAT) and (AZ_MIN_LNG <= lng <= AZ_MAX_LNG)\n",
        "\n",
        "# optimization function\n",
        "def optimize_data_center_locations(baseline_centers, potential_locations, min_distance=10):\n",
        "    print(f\"Starting optimization with {len(baseline_centers)} data centers and {len(potential_locations)} potential locations\")\n",
        "    np.random.seed(42)\n",
        "\n",
        "    locations = generate_water_stress(potential_locations)\n",
        "    locations = generate_electricity_prices(locations)\n",
        "    locations = generate_carbon_emissions(locations)\n",
        "    locations = generate_water_efficiency(locations)\n",
        "\n",
        "    location_ids = [loc[\"id\"] for loc in locations]\n",
        "    water_stress = {loc[\"id\"]: loc[\"water_stress\"] for loc in locations}\n",
        "    electricity_price = {loc[\"id\"]: loc[\"electricity_price\"] for loc in locations}\n",
        "    carbon_emissions = {loc[\"id\"]: loc[\"carbon_emissions\"] for loc in locations}\n",
        "    water_efficiency = {loc[\"id\"]: loc[\"water_efficiency\"] for loc in locations}\n",
        "\n",
        "    # coords for distance calculations\n",
        "    location_coords = {loc[\"id\"]: (loc[\"lat\"], loc[\"lng\"]) for loc in locations}\n",
        "\n",
        "    # normalization values for the obj function\n",
        "    max_water_stress = max(water_stress.values())\n",
        "    max_electricity_price = max(electricity_price.values())\n",
        "    max_carbon_emissions = max(carbon_emissions.values())\n",
        "    min_water_efficiency = min(water_efficiency.values())\n",
        "    max_water_efficiency = max(water_efficiency.values())\n",
        "\n",
        "    # objective weights, which can be adjusted based on priorities (kind of chose randomly, so if you want to update at any point, feel free :) )\n",
        "    # sounds great! - RICHARD\n",
        "    # my initial reaction is to set water efficiency weight to zero lol.  but i definitely have some biases/leanings, so we can definitely discuss this. - RICHARD\n",
        "    # another interesting question, kind of taking from my UROP last year, is: at what threshold of objective weights, e.g. on water, do we need before it becomes - RICHARD\n",
        "    # unreasonable to place data centers in hot, desert/dry environments? - RICHARD\n",
        "    # I suppose we can try a few different weightings. (water stress, elec, carbon, water efficiency) = (1, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0), (0, 0, 0, 1), (0.35, 0.25, 0.25, 0.15)\n",
        "    weights = {\n",
        "        \"water_stress\": 0.35,\n",
        "        \"electricity_price\": 0.25,\n",
        "        \"carbon_emissions\": 0.25,\n",
        "        \"water_efficiency\": 0.15\n",
        "    }\n",
        "\n",
        "    model = gp.Model(\"DataCenterOptimization\")\n",
        "\n",
        "    # decision variables: x[d,l] = 1 if data center d is placed at location l\n",
        "    x = {}\n",
        "    for d in range(len(baseline_centers)):\n",
        "        for l in location_ids:\n",
        "            x[d, l] = model.addVar(vtype=GRB.BINARY, name=f\"x_{d}_{l}\")\n",
        "\n",
        "    # minimize weighted combination of metrics\n",
        "    obj = gp.LinExpr()\n",
        "\n",
        "    for d in range(len(baseline_centers)):\n",
        "        for l in location_ids:\n",
        "            # note: for water efficiency, higher is better, so we invert it\n",
        "            # out of curiosity, why not 1 - water_efficiency[l]/max_water_efficiency? But works either way! - RICHARD\n",
        "            normalized_water_efficiency = 1 - (water_efficiency[l] - min_water_efficiency) / (max_water_efficiency - min_water_efficiency)\n",
        "\n",
        "            norm_water_stress = water_stress[l] / max_water_stress\n",
        "            norm_electricity = electricity_price[l] / max_electricity_price\n",
        "            norm_emissions = carbon_emissions[l] / max_carbon_emissions\n",
        "\n",
        "\n",
        "            # I believe that relocating a data center costs around $120,000: https://www.google.com/url?q=https://apposite-tech.com/best-practices-data-center-relocation/&sa=D&source=docs&ust=1744170960046669&usg=AOvVaw1nspqunGp5e_Gn6C7xgrJB. - RICHARD\n",
        "            # Perhaps we could also induce a cost for relocating a data center. So you are more reluctant to actually move a data center unless absolutely necessary. - RICHARD\n",
        "\n",
        "            weighted_sum = (\n",
        "                weights[\"water_stress\"] * norm_water_stress +\n",
        "                weights[\"electricity_price\"] * norm_electricity +\n",
        "                weights[\"carbon_emissions\"] * norm_emissions +\n",
        "                weights[\"water_efficiency\"] * normalized_water_efficiency\n",
        "            )\n",
        "\n",
        "            obj += x[d, l] * weighted_sum\n",
        "\n",
        "    model.setObjective(obj, GRB.MINIMIZE)\n",
        "\n",
        "    # each data center must be placed at exactly one location\n",
        "    for d in range(len(baseline_centers)):\n",
        "        model.addConstr(gp.quicksum(x[d, l] for l in location_ids) == 1, f\"one_loc_per_dc_{d}\")\n",
        "\n",
        "    #each location can have at most one data center\n",
        "    for l in location_ids:\n",
        "        model.addConstr(gp.quicksum(x[d, l] for d in range(len(baseline_centers))) <= 1, f\"one_dc_per_loc_{l}\")\n",
        "\n",
        "    # idk if we need this, but its basically setting a minimum distance between data centers since i dont want the model spitting them out on top of eachy other\n",
        "    # Ooo great point. I feel like this gets at our conversation about why you would ever consider clustering data centers. - RICHARD\n",
        "    # However, I wonder if the water scarcity aspect will naturally cause the model to not cram all the data centers together, if they\n",
        "    # start to pile on too much burden on the local water system. - RICHARD\n",
        "    # Anyhow, I believe the fact that you set the locations in a grid pattern at least gives some minimum spacing. We can keep talking\n",
        "    # about this more if needed! - RICHARD\n",
        "    if min_distance > 0:\n",
        "        location_distances = {}\n",
        "        for l1 in location_ids:\n",
        "            for l2 in location_ids:\n",
        "                if l1 < l2:\n",
        "                    lat1, lng1 = location_coords[l1]\n",
        "                    lat2, lng2 = location_coords[l2]\n",
        "                    dist = calc_distance(lat1, lng1, lat2, lng2)\n",
        "                    location_distances[(l1, l2)] = dist\n",
        "                    location_distances[(l2, l1)] = dist\n",
        "\n",
        "        # so basically, for locations that are too close in lat/lon, prevent placing data centers at both\n",
        "        # I wonder if the below step is contributing to the computational bottleneck. But we don't have to worry about it if it runs\n",
        "        # well on your computer locally :) - RICHARD\n",
        "        for (l1, l2), dist in location_distances.items():\n",
        "            if dist < min_distance:\n",
        "                for d1 in range(len(baseline_centers)):\n",
        "                    for d2 in range(d1 + 1, len(baseline_centers)):\n",
        "                        model.addConstr(x[d1, l1] + x[d2, l2] <= 1, f\"min_dist_{d1}_{l1}_{d2}_{l2}\")\n",
        "\n",
        "    model.optimize()\n",
        "\n",
        "    if model.status == GRB.OPTIMAL:\n",
        "        print(f\"Optimal solution found w/ objective value: {model.objVal}\")\n",
        "\n",
        "        # extract the solution\n",
        "        solution = []\n",
        "        for d in range(len(baseline_centers)):\n",
        "            dc = baseline_centers[d]\n",
        "            for l in location_ids:\n",
        "                if x[d, l].x > 0.5:\n",
        "                    loc_info = next(loc for loc in locations if loc[\"id\"] == l)\n",
        "                    solution.append({\n",
        "                        \"data_center_id\": d,\n",
        "                        \"original_city\": dc[\"city\"],\n",
        "                        \"original_lat\": dc[\"lat\"],\n",
        "                        \"original_lng\": dc[\"lng\"],\n",
        "                        \"new_lat\": loc_info[\"lat\"],\n",
        "                        \"new_lng\": loc_info[\"lng\"],\n",
        "                        \"water_stress\": loc_info[\"water_stress\"],\n",
        "                        \"electricity_price\": loc_info[\"electricity_price\"],\n",
        "                        \"carbon_emissions\": loc_info[\"carbon_emissions\"],\n",
        "                        \"water_efficiency\": loc_info[\"water_efficiency\"],\n",
        "                        \"distance\": calc_distance(\n",
        "                            dc[\"lat\"], dc[\"lng\"],\n",
        "                            loc_info[\"lat\"], loc_info[\"lng\"]\n",
        "                        )\n",
        "                        # feel free to adjujst this list, i was just making sure things seemed okay\n",
        "                    })\n",
        "                    break\n",
        "\n",
        "\n",
        "        # LEFT OFF HERE - RICHARD\n",
        "\n",
        "        baseline_metrics = []\n",
        "        for dc in baseline_centers:\n",
        "            min_dist = float('inf')\n",
        "            closest_loc = None\n",
        "\n",
        "            for loc in locations:\n",
        "                dist = calc_distance(dc[\"lat\"], dc[\"lng\"], loc[\"lat\"], loc[\"lng\"])\n",
        "                if dist < min_dist:\n",
        "                    min_dist = dist\n",
        "                    closest_loc = loc\n",
        "\n",
        "            baseline_metrics.append({\n",
        "                \"city\": dc[\"city\"],\n",
        "                \"lat\": dc[\"lat\"],\n",
        "                \"lng\": dc[\"lng\"],\n",
        "                \"water_stress\": closest_loc[\"water_stress\"],\n",
        "                \"electricity_price\": closest_loc[\"electricity_price\"],\n",
        "                \"carbon_emissions\": closest_loc[\"carbon_emissions\"],\n",
        "                \"water_efficiency\": closest_loc[\"water_efficiency\"]\n",
        "            })\n",
        "\n",
        "        # just to see the average metrics\n",
        "        baseline_avg = {\n",
        "            \"water_stress\": sum(b[\"water_stress\"] for b in baseline_metrics) / len(baseline_metrics),\n",
        "            \"electricity_price\": sum(b[\"electricity_price\"] for b in baseline_metrics) / len(baseline_metrics),\n",
        "            \"carbon_emissions\": sum(b[\"carbon_emissions\"] for b in baseline_metrics) / len(baseline_metrics),\n",
        "            \"water_efficiency\": sum(b[\"water_efficiency\"] for b in baseline_metrics) / len(baseline_metrics)\n",
        "        }\n",
        "\n",
        "        optimized_avg = {\n",
        "            \"water_stress\": sum(s[\"water_stress\"] for s in solution) / len(solution),\n",
        "            \"electricity_price\": sum(s[\"electricity_price\"] for s in solution) / len(solution),\n",
        "            \"carbon_emissions\": sum(s[\"carbon_emissions\"] for s in solution) / len(solution),\n",
        "            \"water_efficiency\": sum(s[\"water_efficiency\"] for s in solution) / len(solution),\n",
        "            \"avg_distance\": sum(s[\"distance\"] for s in solution) / len(solution)\n",
        "        }\n",
        "\n",
        "        # here are the metrics compared to the original ben chmark\n",
        "        print(\"\\nMetrics Comparison:\")\n",
        "        print(f\"Water Stress: {baseline_avg['water_stress']:.2f} → {optimized_avg['water_stress']:.2f} \" +\n",
        "              f\"({((baseline_avg['water_stress'] - optimized_avg['water_stress']) / baseline_avg['water_stress'] * 100):.2f}% improvement)\")\n",
        "\n",
        "        print(f\"Electricity Price: ${baseline_avg['electricity_price']:.4f}/kWh → ${optimized_avg['electricity_price']:.4f}/kWh \" +\n",
        "              f\"({((baseline_avg['electricity_price'] - optimized_avg['electricity_price']) / baseline_avg['electricity_price'] * 100):.2f}% improvement)\")\n",
        "\n",
        "        print(f\"Carbon Emissions: {baseline_avg['carbon_emissions']:.4f} tons CO2/MWh → {optimized_avg['carbon_emissions']:.4f} tons CO2/MWh \" +\n",
        "              f\"({((baseline_avg['carbon_emissions'] - optimized_avg['carbon_emissions']) / baseline_avg['carbon_emissions'] * 100):.2f}% improvement)\")\n",
        "\n",
        "        print(f\"Water Efficiency: {baseline_avg['water_efficiency']:.2f} → {optimized_avg['water_efficiency']:.2f} \" +\n",
        "              f\"({((optimized_avg['water_efficiency'] - baseline_avg['water_efficiency']) / baseline_avg['water_efficiency'] * 100):.2f}% improvement)\")\n",
        "\n",
        "        print(f\"Average Relocation Distance: {optimized_avg['avg_distance']:.2f} km\")\n",
        "\n",
        "        return {\n",
        "            \"objective_value\": model.objVal,\n",
        "            \"solution\": solution,\n",
        "            \"baseline\": baseline_metrics,\n",
        "            \"metrics\": {\n",
        "                \"baseline\": baseline_avg,\n",
        "                \"optimized\": optimized_avg\n",
        "            }\n",
        "        }\n",
        "    else:\n",
        "        print(f\"No optimal solution found. Status: {model.status}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "# \\main execution\n",
        "if __name__ == \"__main__\":\n",
        "  #grid resoljution\n",
        "    resolution = 0.25  # 0.25 degrees = ~28 km\n",
        "\n",
        "    # adjust the spacing based on longitude (since longitude degrees are smaller at higher latitudes)\n",
        "    # again, this is from my old UROP, so could be slightly off\n",
        "    avg_lat = (AZ_MIN_LAT + AZ_MAX_LAT) / 2\n",
        "    lng_spacing = resolution / np.cos(np.radians(avg_lat))\n",
        "\n",
        "    # create baseline data centers (129 total for az)\n",
        "    baseline_centers = create_baseline_centers()\n",
        "    print(f\"Created {len(baseline_centers)} baseline data centers\")\n",
        "\n",
        "    # make a grid of potential locations\n",
        "    potential_locations = create_location_grid(resolution)\n",
        "    print(f\"Created {len(potential_locations)} potential locations\")\n",
        "\n",
        "    # minimum distance between data centers (in km)\n",
        "    # just set it to 0 to disable this constraint\n",
        "    min_distance = 20  # km\n",
        "\n",
        "    results = optimize_data_center_locations(baseline_centers, potential_locations, min_distance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HxFfgSQHAYo",
        "outputId": "b2d76246-a160-4b31-8c6b-88cdf33ef4ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gurobipy\n",
            "  Downloading gurobipy-12.0.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (16 kB)\n",
            "Downloading gurobipy-12.0.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (14.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.4/14.4 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gurobipy\n",
            "Successfully installed gurobipy-12.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gurobipy"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
