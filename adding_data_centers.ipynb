{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import gurobipy as gp\n",
        "from gurobipy import GRB\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from scipy.interpolate import griddata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  super().__setitem__(key, value)\n",
            "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  super().__setitem__(key, value)\n",
            "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  super().__setitem__(key, value)\n"
          ]
        }
      ],
      "source": [
        "# Arizona shapefile\n",
        "\n",
        "us_states = gpd.read_file('/Users/richy/Downloads/Classes/Spring_2025/1.020/Project/cb_2018_us_state_500k/cb_2018_us_state_500k.shp')\n",
        "arizona = us_states[us_states['NAME'] == 'Arizona']\n",
        "arizona.to_crs(\"EPSG:5070\", inplace=True) # we use EPSG 5070 to get more accurate area measurements. https://epsg.io/5070-1252\n",
        "\n",
        "# water basins shapefile\n",
        "\n",
        "huc_subbasins = gpd.read_file('/Users/richy/Downloads/Classes/Spring_2025/1.020/Project/HUC8_CONUS/HUC8_US.shp') # read in subbasins\n",
        "huc_subbasins['has_arizona'] = huc_subbasins['STATES'].map(lambda x: 'AZ' in x)\n",
        "huc_subbasins_az = huc_subbasins[huc_subbasins['has_arizona']]\n",
        "huc_subbasins_az.to_crs(\"EPSG:5070\", inplace=True)\n",
        "\n",
        "huc_subbasins_az['HUC6'] = huc_subbasins_az['HUC8'].map(lambda x: x[:6])\n",
        "\n",
        "# also, in case we want centroid:\n",
        "# huc_subbasins_az['centroid'] = huc_subbasins_az.to_crs(\"+proj=cea\").centroid.to_crs(huc_subbasins_az.crs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get lists of HUC8 subbasins that group into HUC6 basins\n",
        "huc6_huc8_dict = dict()\n",
        "\n",
        "for huc6 in huc_subbasins_az['HUC6'].unique():\n",
        "    huc_filter = huc_subbasins_az[huc_subbasins_az['HUC6'] == huc6]\n",
        "    huc6_huc8_dict[huc6] = huc_filter['HUC8'].to_list()\n",
        "\n",
        "# data values for HUC8 regions generated from the paper by Abu Bakar Siddik et al 2021: https://iopscience.iop.org/article/10.1088/1748-9326/abfba1?_sp=b48260d8-0a7b-4784-9d4b-0e1ac60ee727\n",
        "water_carbon = pd.read_excel(\"/Users/richy/Downloads/Classes/Spring_2025/1.020/Project/DC_footprint/SI_XLS/Results.xlsx\", sheet_name=\"Table 3\", skiprows=1)\n",
        "\n",
        "water_carbon['HUC8_str'] = water_carbon['HUC8'].map(lambda x: ''.join(['0']*(8-len(str(x)))) + str(x))\n",
        "\n",
        "# filter to only HUC rows\n",
        "water_carbon['not_nan'] = ~water_carbon['WSF_1MW_DC'].isna()\n",
        "water_carbon = water_carbon[water_carbon['not_nan']]\n",
        "\n",
        "# now merge with the arizona HUC data\n",
        "huc_subbasins_az_join = huc_subbasins_az.merge(water_carbon, how=\"inner\", left_on=\"HUC8\", right_on=\"HUC8_str\")\n",
        "huc_subbasins_az_join = gpd.clip(huc_subbasins_az_join, arizona)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# take a look at water stress factor, carbon factor data\n",
        "wsf_dict = {k: v for k, v in zip(huc_subbasins_az_join['HUC8_y'], huc_subbasins_az_join['WSF_1MW_DC'])}\n",
        "cf_dict = {k: v for k, v in zip(huc_subbasins_az_join['HUC8_y'], huc_subbasins_az_join['CF_1MW_DC'])}\n",
        "\n",
        "s_max = huc_subbasins_az_join['WSF_1MW_DC'].max()\n",
        "e_max = huc_subbasins_az_join['CF_1MW_DC'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# estimations of existing data centers: https://www.datacentermap.com/\n",
        "# power of existing data centers? Unclear. https://baxtel.com/data-center/phoenix\n",
        "# well, in total there is around 602.8 MW in the Phoenix area. https://azbigmedia.com/real-estate/phoenix-ranks-4th-among-north-american-data-center-markets/\n",
        "# and, in the Phoenix area there are 124 data centers. https://www.datacentermap.com/usa/arizona/\n",
        "# so, let's say on average, each existing data center takes on 5 MW of load.\n",
        "\n",
        "# existing inventory HUC8\n",
        "# 15050301: 6\n",
        "# 15050302: 2\n",
        "# 15050303: 4 (from Eloy) \n",
        "# 15050100: 17 (from Mesa airport AZA) 13 (from Chandler airport CHD) 4 (from Gilbert)\n",
        "# 15060106: 2 (from Mesa) 25 (from Phoenix airport and Scottsdale) 2 (from upper Scottsdale)\n",
        "# 15070102: 6 (from Deer Valley) 40 (from Litchfield park airport)\n",
        "# 15070103: 1 (from Buckeye/Arlington)\n",
        "\n",
        "\n",
        "# totals (done by hand, because data lies behind paywalls):\n",
        "# 15050301: 6\n",
        "# 15050302: 2\n",
        "# 15050303: 4\n",
        "# 15050100: 34\n",
        "# 15060106: 29\n",
        "# 15070102: 46\n",
        "# 15070103: 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  super().__setitem__(key, value)\n"
          ]
        }
      ],
      "source": [
        "# electricity prices\n",
        "# https://www.energysage.com/local-data/electricity-cost/az/\n",
        "# cochise county: 16 cents/kwh\n",
        "# coconino county: 15 cents/kwh\n",
        "# gila county: 20 cents/kwh\n",
        "# la paz county: 15 cents/kwh\n",
        "# maricopa county: 15 cents/kwh\n",
        "# mohave county: 18 cents/kwh\n",
        "# navajo county: 15 cents/kwh\n",
        "# pima county: 18 cents/kwh\n",
        "# pinal county: 16 cents/kwh\n",
        "# yavapai county: 15 cents/kwh\n",
        "# yuma county: 15 cents/kwh\n",
        "\n",
        "us_counties = gpd.read_file('/Users/richy/Downloads/Classes/Spring_2025/1.020/Project/cb_2018_us_county_500k/cb_2018_us_county_500k.shp') # read in counties\n",
        "arizona_counties = us_counties[us_counties['STATEFP'] == '04'] # filter to arizona\n",
        "arizona_counties.to_crs(\"EPSG:5070\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  super().__setitem__(key, value)\n",
            "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  super().__setitem__(key, value)\n"
          ]
        }
      ],
      "source": [
        "# units: cents/kwh\n",
        "elec_price_dict = {'Cochise': 16, 'Coconino': 15, 'Gila': 20, 'La Paz': 15, 'Maricopa': 15, 'Mohave': 18, \n",
        "                   'Navajo': 15, 'Pima': 18, 'Pinal': 16, 'Yavapai': 15, 'Yuma': 15}\n",
        "\n",
        "# new units: dollars/mw = cents/kwh * (0.01 dollar/1 cent) * (1000 kwh/1 mw)\n",
        "elec_price_dict = {k: 10*v for k, v in elec_price_dict.items()}\n",
        "\n",
        "has_elec_price = set(elec_price_dict.keys())\n",
        "num_elec_price = len(has_elec_price)\n",
        "\n",
        "# find arizona counties with electricity price data\n",
        "arizona_counties['has_elec_price'] = arizona_counties['NAME'].map(lambda x: x in has_elec_price)\n",
        "arizona_counties_elec_price = arizona_counties[arizona_counties['has_elec_price']]\n",
        "arizona_counties_elec_price['elec_price'] = arizona_counties_elec_price['NAME'].map(lambda x: elec_price_dict[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class resolve_regions:\n",
        "    \"\"\" \n",
        "    Converts one subdivision of space (e.g., county) into another subdivision of space (e.g., HUC8.)\n",
        "\n",
        "    Averages out value of interest according to amount of overlap with the first subdivision of space, or the \n",
        "    closest region. \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, from_df, to_df, value_col, id_row):\n",
        "        \"\"\" \n",
        "        Init method for the class resolve_regions.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            from_df: gpd.GeoDataFrame\n",
        "                Dataframe to convert from\n",
        "            to_df: gpd.GeoDataFrame\n",
        "                Dataframe to convert to\n",
        "            value_col: string\n",
        "                Name of column with values of interest\n",
        "            id_row: string\n",
        "                Name of column with unique identifiers, in to_df\n",
        "        \"\"\"\n",
        "\n",
        "        self.from_df = from_df\n",
        "        self.to_df = to_df\n",
        "        self.value_col = value_col\n",
        "        self.id_row = id_row\n",
        "\n",
        "# TODO: currently this only handles average. We can also functionalize it to handle other functions.\n",
        "    def weighted_computation(self, id):\n",
        "        \"\"\" \n",
        "        Perform the weighted average computation for the output row with identification id.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            id: string\n",
        "                Unique ID for the output row\n",
        "        \"\"\"\n",
        "\n",
        "        to_geometry = self.to_df[self.to_df[self.id_row] == id]['geometry'].iloc[0] # extract geometry of to_df\n",
        "        from_to_intersect = self.from_df['geometry'].intersection(to_geometry) # obtain intersections\n",
        "\n",
        "        area_sum = np.sum(from_to_intersect.area)\n",
        "\n",
        "        if area_sum > 0: # some intersection\n",
        "            weighted_val = 0 # keep track of weighted sum for value of interest\n",
        "\n",
        "            for idx, area in from_to_intersect.area.items():\n",
        "                weighted_val += self.from_df.loc[idx, self.value_col]*area\n",
        "            \n",
        "            weighted_val /= area_sum\n",
        "        \n",
        "        else:\n",
        "            from_to_distance = self.from_df['geometry'].distance(to_geometry) # obtain distances\n",
        "            from_to_distance.sort_values(ascending=True, inplace=True)\n",
        "            lowest_idx = from_to_distance.index[0]\n",
        "\n",
        "            weighted_val = self.from_df.loc[lowest_idx, self.value_col]\n",
        "\n",
        "        return weighted_val\n",
        "\n",
        "    def convert_regions(self):\n",
        "        \"\"\" \n",
        "        Perform conversion between the subdivisions of space, and average the value of interest.\n",
        "        \"\"\"\n",
        "\n",
        "        converted_value_list = []\n",
        "\n",
        "        for id in self.to_df[self.id_row]: # iterate over geometries of to_df\n",
        "            converted_value_list.append(self.weighted_computation(id))\n",
        "\n",
        "        self.to_df[self.value_col] = converted_value_list # set the output values after conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# assign electricity price in HUC8 regions to closest county with price\n",
        "# consider: if desired, we can add some random noise\n",
        "\n",
        "# build up the elec_price ($/MW) column\n",
        "\n",
        "county_to_huc_elec_price = resolve_regions(arizona_counties_elec_price, huc_subbasins_az_join, 'elec_price', 'HUC8_y')\n",
        "county_to_huc_elec_price.convert_regions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# next, let's take a look at land prices. \n",
        "# we get TIF files from here: https://www.pnas.org/doi/10.1073/pnas.2012865117. Units: log(2017$/hectare)\n",
        "# first, let's convert into linear units.\n",
        "\n",
        "from osgeo import gdal\n",
        "\n",
        "import rasterio\n",
        "\n",
        "# perhaps we can use this for editing the tif file: https://www.luisalucchese.com/post/open-edit-save-raster-files-using-python/\n",
        "# source: https://www.luisalucchese.com/post/open-edit-save-raster-files-using-python/\n",
        "\n",
        "# convert $/hectare to $/acre\n",
        "acre_to_hectare = 2.47105\n",
        "\n",
        "# original data is in ln(2017$/hectare). Transform to linear scaling\n",
        "land_vacant = rasterio.open('/Users/richy/Downloads/Classes/Spring_2025/1.020/Project/doi_10_5061_dryad_np5hqbzq9__v20201008/places_fmv_pnas_dryad/1 estimates/places_fmv_vacant.tif')\n",
        "land_data_np = land_vacant.read(1)\n",
        "land_vacant.close()\n",
        "\n",
        "land_data_np = np.where(land_data_np != 0, np.exp(land_data_np), 0) # transform the data to 2017$/hectare\n",
        "land_data_np = land_data_np/acre_to_hectare # transform the data to 2017$/acre\n",
        "\n",
        "# save back to tif\n",
        "with rasterio.open('/Users/richy/Downloads/Classes/Spring_2025/1.020/Project/doi_10_5061_dryad_np5hqbzq9__v20201008/places_fmv_pnas_dryad/1 estimates/land_cost_linear.tif', 'r+') as f:\n",
        "    f.write(land_data_np, indexes=1)\n",
        "\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>STATEFP</th>\n",
              "      <th>STATENS</th>\n",
              "      <th>AFFGEOID</th>\n",
              "      <th>GEOID</th>\n",
              "      <th>STUSPS</th>\n",
              "      <th>NAME</th>\n",
              "      <th>LSAD</th>\n",
              "      <th>ALAND</th>\n",
              "      <th>AWATER</th>\n",
              "      <th>geometry</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>04</td>\n",
              "      <td>01779777</td>\n",
              "      <td>0400000US04</td>\n",
              "      <td>04</td>\n",
              "      <td>AZ</td>\n",
              "      <td>Arizona</td>\n",
              "      <td>00</td>\n",
              "      <td>294198551143</td>\n",
              "      <td>1027337603</td>\n",
              "      <td>POLYGON ((-1746851.979 1221914.112, -1746649.6...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   STATEFP   STATENS     AFFGEOID GEOID STUSPS     NAME LSAD         ALAND  \\\n",
              "35      04  01779777  0400000US04    04     AZ  Arizona   00  294198551143   \n",
              "\n",
              "        AWATER                                           geometry  \n",
              "35  1027337603  POLYGON ((-1746851.979 1221914.112, -1746649.6...  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "arizona"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "# now, we resample the raster first to avoid high compute times: Raster > Projections > Warp (Reproject). We resample using median, so that we can account for/ignore outliers: really expensive pieces of land (ex. Manhattan).\n",
        "# we then convert raster to polygon using QGIS. Raster > Conversion > Polygonize (Raster to Vector)\n",
        "\n",
        "# import the converted polygon file\n",
        "land_costs_shp = gpd.read_file('/Users/richy/Downloads/Classes/Spring_2025/1.020/Project/doi_10_5061_dryad_np5hqbzq9__v20201008/places_fmv_pnas_dryad/1 estimates/land_cost_linear_10000x10000.shp')\n",
        "land_costs_shp.to_crs('EPSG:5070', inplace=True)\n",
        "land_costs_shp_az = land_costs_shp[land_costs_shp.intersects(arizona.iloc[0,-1])]\n",
        "\n",
        "# NOTE: to increase accuracy, make sure to account for public land and Native American land. https://www.pnas.org/doi/10.1073/pnas.2012865117"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute estimated land costs in each HUC8\n",
        "pixel_to_huc_land_cost = resolve_regions(land_costs_shp, huc_subbasins_az_join, 'DN', 'HUC8_y')\n",
        "pixel_to_huc_land_cost.convert_regions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TNMID</th>\n",
              "      <th>METASOURCE</th>\n",
              "      <th>SOURCEDATA</th>\n",
              "      <th>SOURCEORIG</th>\n",
              "      <th>SOURCEFEAT</th>\n",
              "      <th>LOADDATE</th>\n",
              "      <th>GNIS_ID</th>\n",
              "      <th>AREAACRES</th>\n",
              "      <th>AREASQKM</th>\n",
              "      <th>STATES</th>\n",
              "      <th>...</th>\n",
              "      <th>has_arizona</th>\n",
              "      <th>HUC6</th>\n",
              "      <th>HUC8_y</th>\n",
              "      <th>WSF_1MW_DC</th>\n",
              "      <th>WF_1MW_DC</th>\n",
              "      <th>CF_1MW_DC</th>\n",
              "      <th>HUC8_str</th>\n",
              "      <th>not_nan</th>\n",
              "      <th>elec_price</th>\n",
              "      <th>DN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>{A078F6A3-1492-4880-AB0B-556324E832FA}</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2012/06/11</td>\n",
              "      <td>0</td>\n",
              "      <td>6358140.45</td>\n",
              "      <td>25730.50</td>\n",
              "      <td>AZ,MX</td>\n",
              "      <td>...</td>\n",
              "      <td>True</td>\n",
              "      <td>150802</td>\n",
              "      <td>15080200</td>\n",
              "      <td>27.483282</td>\n",
              "      <td>105.972943</td>\n",
              "      <td>0.194845</td>\n",
              "      <td>15080200</td>\n",
              "      <td>True</td>\n",
              "      <td>180.000000</td>\n",
              "      <td>2153.435317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>{21A6AE9C-8C85-4578-AE5A-368093D0E9E5}</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2012/06/11</td>\n",
              "      <td>0</td>\n",
              "      <td>1587400.21</td>\n",
              "      <td>6423.99</td>\n",
              "      <td>AZ,MX</td>\n",
              "      <td>...</td>\n",
              "      <td>True</td>\n",
              "      <td>150502</td>\n",
              "      <td>15050202</td>\n",
              "      <td>36.267861</td>\n",
              "      <td>105.972943</td>\n",
              "      <td>0.194845</td>\n",
              "      <td>15050202</td>\n",
              "      <td>True</td>\n",
              "      <td>160.277137</td>\n",
              "      <td>3507.698389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>{EE94F429-A5EE-4583-8558-00435EF322A7}</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2012/06/11</td>\n",
              "      <td>0</td>\n",
              "      <td>1680515.46</td>\n",
              "      <td>6800.81</td>\n",
              "      <td>AZ,MX</td>\n",
              "      <td>...</td>\n",
              "      <td>True</td>\n",
              "      <td>150503</td>\n",
              "      <td>15050301</td>\n",
              "      <td>70.305819</td>\n",
              "      <td>52.634425</td>\n",
              "      <td>0.529667</td>\n",
              "      <td>15050301</td>\n",
              "      <td>True</td>\n",
              "      <td>177.606132</td>\n",
              "      <td>13978.041516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>{245E1BED-9BDD-401E-8361-9EC35F6E387E}</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2012/06/11</td>\n",
              "      <td>0</td>\n",
              "      <td>589332.15</td>\n",
              "      <td>2384.94</td>\n",
              "      <td>AZ</td>\n",
              "      <td>...</td>\n",
              "      <td>True</td>\n",
              "      <td>150503</td>\n",
              "      <td>15050302</td>\n",
              "      <td>56.333972</td>\n",
              "      <td>39.854302</td>\n",
              "      <td>0.639959</td>\n",
              "      <td>15050302</td>\n",
              "      <td>True</td>\n",
              "      <td>179.377847</td>\n",
              "      <td>22129.871558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>{54769F22-97B4-415B-BC91-102E24FA3768}</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2012/06/11</td>\n",
              "      <td>0</td>\n",
              "      <td>901722.02</td>\n",
              "      <td>3649.14</td>\n",
              "      <td>AZ</td>\n",
              "      <td>...</td>\n",
              "      <td>True</td>\n",
              "      <td>150503</td>\n",
              "      <td>15050304</td>\n",
              "      <td>115.938523</td>\n",
              "      <td>52.634425</td>\n",
              "      <td>0.529667</td>\n",
              "      <td>15050304</td>\n",
              "      <td>True</td>\n",
              "      <td>179.886932</td>\n",
              "      <td>4195.377604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>{E0D05FB2-3A64-4F1C-AD4C-D81FE6BDFD00}</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2012/06/11</td>\n",
              "      <td>0</td>\n",
              "      <td>1697235.34</td>\n",
              "      <td>6868.47</td>\n",
              "      <td>AZ</td>\n",
              "      <td>...</td>\n",
              "      <td>True</td>\n",
              "      <td>150200</td>\n",
              "      <td>15020018</td>\n",
              "      <td>256.903962</td>\n",
              "      <td>25.778796</td>\n",
              "      <td>0.703889</td>\n",
              "      <td>15020018</td>\n",
              "      <td>True</td>\n",
              "      <td>150.000000</td>\n",
              "      <td>982.598826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>{B0F45574-8471-4A73-95F2-582A22BD57D9}</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2012/06/11</td>\n",
              "      <td>0</td>\n",
              "      <td>939702.77</td>\n",
              "      <td>3802.85</td>\n",
              "      <td>AZ</td>\n",
              "      <td>...</td>\n",
              "      <td>True</td>\n",
              "      <td>150100</td>\n",
              "      <td>15010001</td>\n",
              "      <td>77.083962</td>\n",
              "      <td>25.778796</td>\n",
              "      <td>0.703889</td>\n",
              "      <td>15010001</td>\n",
              "      <td>True</td>\n",
              "      <td>150.000000</td>\n",
              "      <td>2440.291677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>{50D2624C-5AA1-4731-AC1F-14A1D6271ABA}</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2012/06/11</td>\n",
              "      <td>0</td>\n",
              "      <td>1879431.59</td>\n",
              "      <td>7605.80</td>\n",
              "      <td>AZ,UT</td>\n",
              "      <td>...</td>\n",
              "      <td>True</td>\n",
              "      <td>140700</td>\n",
              "      <td>14070006</td>\n",
              "      <td>50.140425</td>\n",
              "      <td>17.562764</td>\n",
              "      <td>0.814824</td>\n",
              "      <td>14070006</td>\n",
              "      <td>True</td>\n",
              "      <td>150.000000</td>\n",
              "      <td>1950.020518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>{465950E6-588A-4AED-BDD9-1B244FBAA5BF}</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2012/06/11</td>\n",
              "      <td>0</td>\n",
              "      <td>1512613.38</td>\n",
              "      <td>6121.33</td>\n",
              "      <td>AZ,UT</td>\n",
              "      <td>...</td>\n",
              "      <td>True</td>\n",
              "      <td>150100</td>\n",
              "      <td>15010003</td>\n",
              "      <td>94.884359</td>\n",
              "      <td>25.778796</td>\n",
              "      <td>0.703889</td>\n",
              "      <td>15010003</td>\n",
              "      <td>True</td>\n",
              "      <td>162.538478</td>\n",
              "      <td>2441.731983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>{EEAAE222-A384-4065-A138-39633E91FEA4}</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2012/06/11</td>\n",
              "      <td>0</td>\n",
              "      <td>908348.94</td>\n",
              "      <td>3675.96</td>\n",
              "      <td>AZ,UT</td>\n",
              "      <td>...</td>\n",
              "      <td>True</td>\n",
              "      <td>140700</td>\n",
              "      <td>14070007</td>\n",
              "      <td>87.122303</td>\n",
              "      <td>105.972943</td>\n",
              "      <td>0.194845</td>\n",
              "      <td>14070007</td>\n",
              "      <td>True</td>\n",
              "      <td>150.000000</td>\n",
              "      <td>1733.574145</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>84 rows × 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     TNMID METASOURCE SOURCEDATA SOURCEORIG  \\\n",
              "67  {A078F6A3-1492-4880-AB0B-556324E832FA}       None       None       None   \n",
              "63  {21A6AE9C-8C85-4578-AE5A-368093D0E9E5}       None       None       None   \n",
              "40  {EE94F429-A5EE-4583-8558-00435EF322A7}       None       None       None   \n",
              "41  {245E1BED-9BDD-401E-8361-9EC35F6E387E}       None       None       None   \n",
              "43  {54769F22-97B4-415B-BC91-102E24FA3768}       None       None       None   \n",
              "..                                     ...        ...        ...        ...   \n",
              "27  {E0D05FB2-3A64-4F1C-AD4C-D81FE6BDFD00}       None       None       None   \n",
              "7   {B0F45574-8471-4A73-95F2-582A22BD57D9}       None       None       None   \n",
              "75  {50D2624C-5AA1-4731-AC1F-14A1D6271ABA}       None       None       None   \n",
              "79  {465950E6-588A-4AED-BDD9-1B244FBAA5BF}       None       None       None   \n",
              "76  {EEAAE222-A384-4065-A138-39633E91FEA4}       None       None       None   \n",
              "\n",
              "   SOURCEFEAT    LOADDATE  GNIS_ID   AREAACRES  AREASQKM STATES  ...  \\\n",
              "67       None  2012/06/11        0  6358140.45  25730.50  AZ,MX  ...   \n",
              "63       None  2012/06/11        0  1587400.21   6423.99  AZ,MX  ...   \n",
              "40       None  2012/06/11        0  1680515.46   6800.81  AZ,MX  ...   \n",
              "41       None  2012/06/11        0   589332.15   2384.94     AZ  ...   \n",
              "43       None  2012/06/11        0   901722.02   3649.14     AZ  ...   \n",
              "..        ...         ...      ...         ...       ...    ...  ...   \n",
              "27       None  2012/06/11        0  1697235.34   6868.47     AZ  ...   \n",
              "7        None  2012/06/11        0   939702.77   3802.85     AZ  ...   \n",
              "75       None  2012/06/11        0  1879431.59   7605.80  AZ,UT  ...   \n",
              "79       None  2012/06/11        0  1512613.38   6121.33  AZ,UT  ...   \n",
              "76       None  2012/06/11        0   908348.94   3675.96  AZ,UT  ...   \n",
              "\n",
              "   has_arizona    HUC6    HUC8_y  WSF_1MW_DC   WF_1MW_DC  CF_1MW_DC  HUC8_str  \\\n",
              "67        True  150802  15080200   27.483282  105.972943   0.194845  15080200   \n",
              "63        True  150502  15050202   36.267861  105.972943   0.194845  15050202   \n",
              "40        True  150503  15050301   70.305819   52.634425   0.529667  15050301   \n",
              "41        True  150503  15050302   56.333972   39.854302   0.639959  15050302   \n",
              "43        True  150503  15050304  115.938523   52.634425   0.529667  15050304   \n",
              "..         ...     ...       ...         ...         ...        ...       ...   \n",
              "27        True  150200  15020018  256.903962   25.778796   0.703889  15020018   \n",
              "7         True  150100  15010001   77.083962   25.778796   0.703889  15010001   \n",
              "75        True  140700  14070006   50.140425   17.562764   0.814824  14070006   \n",
              "79        True  150100  15010003   94.884359   25.778796   0.703889  15010003   \n",
              "76        True  140700  14070007   87.122303  105.972943   0.194845  14070007   \n",
              "\n",
              "   not_nan  elec_price            DN  \n",
              "67    True  180.000000   2153.435317  \n",
              "63    True  160.277137   3507.698389  \n",
              "40    True  177.606132  13978.041516  \n",
              "41    True  179.377847  22129.871558  \n",
              "43    True  179.886932   4195.377604  \n",
              "..     ...         ...           ...  \n",
              "27    True  150.000000    982.598826  \n",
              "7     True  150.000000   2440.291677  \n",
              "75    True  150.000000   1950.020518  \n",
              "79    True  162.538478   2441.731983  \n",
              "76    True  150.000000   1733.574145  \n",
              "\n",
              "[84 rows x 25 columns]"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# requirement for acres of land: https://www.jll.com/en-us/insights/how-to-assess-a-propertys-data-center-potential#:~:text=Data%20centers%20built%20for%20AI,multiple%20power%20substations%20on%20site.\n",
        "# lets say 1 GW = 1000MW requires 500 acres. So each MW requires 0.5 acres.\n",
        "huc_subbasins_az_join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "x5h-jYTzfz_g",
        "outputId": "37a189ed-20d4-4eaf-ca5e-449062417254"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 129 baseline data centers\n",
            "Created 480 potential locations\n",
            "Starting optimization with 129 data centers and 480 potential locations\n",
            "Restricted license - for non-production use only - expires 2026-11-23\n",
            "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (linux64 - \"Ubuntu 22.04.4 LTS\")\n",
            "\n",
            "CPU model: Intel(R) Xeon(R) CPU @ 2.20GHz, instruction set [SSE2|AVX|AVX2]\n",
            "Thread count: 1 physical cores, 2 logical processors, using up to 2 threads\n",
            "\n"
          ]
        },
        {
          "ename": "GurobiError",
          "evalue": "Model too large for size-limited license; visit https://gurobi.com/unrestricted for more information",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mGurobiError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-fa8e945795f4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0mmin_distance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m  \u001b[0;31m# km\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize_data_center_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_centers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpotential_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_distance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-fa8e945795f4>\u001b[0m in \u001b[0;36moptimize_data_center_locations\u001b[0;34m(baseline_centers, potential_locations, min_distance)\u001b[0m\n\u001b[1;32m    345\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddConstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"min_dist_{d1}_{l1}_{d2}_{l2}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mGRB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMAL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32msrc/gurobipy/_model.pyx\u001b[0m in \u001b[0;36mgurobipy._model.Model.optimize\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mGurobiError\u001b[0m: Model too large for size-limited license; visit https://gurobi.com/unrestricted for more information"
          ]
        }
      ],
      "source": [
        "# major cities for baseline data centers\n",
        "# basically, atm all of the data centers are in these three, so im approximating by just using their central lat/lon\n",
        "# count is how the 129 are split among the locations\n",
        "CITIES = {\n",
        "    \"Phoenix\": {\"lat\": 33.4484, \"lng\": -112.0740, \"count\": 121},\n",
        "    \"Tucson\": {\"lat\": 32.2226, \"lng\": -110.9747, \"count\": 7},\n",
        "    \"Nogales\": {\"lat\": 31.3405, \"lng\": -110.9420, \"count\": 1}\n",
        "}\n",
        "\n",
        "# create the baseline data centers locations\n",
        "def create_baseline_centers():\n",
        "    centers = []\n",
        "    center_id = 0\n",
        "\n",
        "    for city, info in CITIES.items():\n",
        "        # decided to add a slight random variation to prevent all centers being at exactly the same point\n",
        "        for i in range(info[\"count\"]):\n",
        "            #within ~5km of the city center\n",
        "            lat_variation = (np.random.random() - 0.5) * 0.05\n",
        "            lng_variation = (np.random.random() - 0.5) * 0.05\n",
        "\n",
        "            centers.append({\n",
        "                \"id\": center_id,\n",
        "                \"city\": city,\n",
        "                \"lat\": info[\"lat\"] + lat_variation,\n",
        "                \"lng\": info[\"lng\"] + lng_variation\n",
        "            })\n",
        "            center_id += 1\n",
        "\n",
        "    return centers\n",
        "\n",
        "# here, i made a grid of potential locations across the approximate Arizona\n",
        "def create_location_grid(resolution=0.2):\n",
        "    locations = []\n",
        "    loc_id = 0\n",
        "    for lat in np.arange(AZ_MIN_LAT, AZ_MAX_LAT + resolution, resolution):\n",
        "        for lng in np.arange(AZ_MIN_LNG, AZ_MAX_LNG + resolution, lng_spacing): # should this be resolution rather than lng_spacing? - RICHARD\n",
        "            locations.append({\n",
        "                \"id\": loc_id,\n",
        "                \"lat\": lat,\n",
        "                \"lng\": lng\n",
        "            })\n",
        "            loc_id += 1\n",
        "\n",
        "    return locations\n",
        "\n",
        "# generate water stress data based on the Aqueduct data approximately\n",
        "# will have to request the actual shapefiles for later on\n",
        "# so this is just simplified, with higher values = more stress (0-5 scale)\n",
        "def generate_water_stress(locations):\n",
        "\n",
        "    # northern Arizona: lower stress (more precipitation)\n",
        "    # central & Phoenix metro: high stress\n",
        "    # southern Arizona: very high stress\n",
        "    # western Arizona along colorado river: medium stress\n",
        "\n",
        "    for loc in locations:\n",
        "        lat, lng = loc[\"lat\"], loc[\"lng\"]\n",
        "        base_stress = 5.0 - (lat - AZ_MIN_LAT) / (AZ_MAX_LAT - AZ_MIN_LAT) * 2.5\n",
        "\n",
        "        # phoenix metro area: very high stress\n",
        "        if (33.0 <= lat <= 34.0) and (-112.5 <= lng <= -111.5):\n",
        "            stress = 4.5 + np.random.random() * 0.5  # 4.5-5.0\n",
        "\n",
        "        # tucson area: very high stress\n",
        "        elif (31.9 <= lat <= 32.5) and (-111.2 <= lng <= -110.7):\n",
        "            stress = 4.2 + np.random.random() * 0.8  # 4.2-5.0\n",
        "\n",
        "        # colorado river region: medium stress due to water access\n",
        "        elif lng < -113.5:\n",
        "            stress = 2.5 + np.random.random() * 1.5  # 2.5-4.0\n",
        "\n",
        "        # northern mountains (e.g., flagstaff): lower stress\n",
        "        elif (34.5 <= lat <= 36.0) and (-112.5 <= lng <= -111.0):\n",
        "            stress = 1.8 + np.random.random() * 1.2  # 1.8-3.0\n",
        "\n",
        "        #  northeastern AZ (like the navajo area): medium-high stress\n",
        "        elif lat > 35.0 and lng > -110.5:\n",
        "            stress = 3.2 + np.random.random() * 1.0  # 3.2-4.2\n",
        "\n",
        "        # if not near, we can just use base stress with some noise\n",
        "        else:\n",
        "            stress = base_stress + (np.random.random() - 0.5) * 1.0\n",
        "\n",
        "        stress = max(0, min(5, stress))\n",
        "        loc[\"water_stress\"] = stress\n",
        "\n",
        "    return locations\n",
        "\n",
        "#electricity price data by county\n",
        "\n",
        "# out of curiosity, where are the electricity prices sourced from? In case we want to reproduce\n",
        "# these electricity price calculations for other states, and I can help - RICHARD\n",
        "def generate_electricity_prices(locations):\n",
        "\n",
        "    # note: the average commercial electricity price in Arizona is ~$0.0811/kWh (8.11 cents)\n",
        "\n",
        "    for loc in locations:\n",
        "        lat, lng = loc[\"lat\"], loc[\"lng\"]\n",
        "        base_price = 0.0811\n",
        "\n",
        "        # phoenix metro area (APS & SRP territory)\n",
        "        if (33.0 <= lat <= 34.0) and (-112.5 <= lng <= -111.5):\n",
        "            if lng < -112.0:  # SRP territory\n",
        "                price = base_price * (0.92 + np.random.random() * 0.08)  # 8% lower on average\n",
        "            else:  # APS territory\n",
        "                price = base_price * (0.98 + np.random.random() * 0.07)  # 2% lower on average\n",
        "\n",
        "        # tucson area (tucson electric power)\n",
        "        elif (31.9 <= lat <= 32.5) and (-111.2 <= lng <= -110.7):\n",
        "            price = base_price * (1.03 + np.random.random() * 0.07)  # 3-10% higher\n",
        "\n",
        "        # rural areas (higher distribution costs)\n",
        "        elif lat > 35.0 or lat < 31.7 or lng < -113.0:\n",
        "            price = base_price * (1.10 + np.random.random() * 0.15)  # 10-25% higher\n",
        "\n",
        "        # otherwise, i use base price with some noise\n",
        "        else:\n",
        "            price = base_price * (0.95 + np.random.random() * 0.15)  # ±10% variation\n",
        "\n",
        "        loc[\"electricity_price\"] = price\n",
        "\n",
        "    return locations\n",
        "\n",
        "# carbon emission rate data in tons Co2/'Mwh\n",
        "def generate_carbon_emissions(locations):\n",
        "\n",
        "    # avg emissions for arizona's grid: ~0.4 tons CO2/MWh (400 kg/MWh)\n",
        "    # from what i looked up, Palo Verde Nuclear Plant creates the biggest variation\n",
        "    # there's probably a better way to do this, but for the approximation, I just use the plant to vary this val plus some of the predominant plant type variation otherwise\n",
        "\n",
        "    for loc in locations:\n",
        "        lat, lng = loc[\"lat\"], loc[\"lng\"]\n",
        "\n",
        "        base_emissions = 0.40  # tons CO2 per MWh\n",
        "\n",
        "        # areas near Palo Verde Nuclear Generating Station (west of phoenix)\n",
        "        if (33.2 <= lat <= 33.5) and (-113.0 <= lng <= -112.5):\n",
        "            emissions = base_emissions * (0.65 + np.random.random() * 0.10)  # 25-35% lower\n",
        "\n",
        "        # phoenix metro area (mixed generation)\n",
        "        elif (33.0 <= lat <= 34.0) and (-112.5 <= lng <= -111.5):\n",
        "            emissions = base_emissions * (0.90 + np.random.random() * 0.15)  # 0-25% lower\n",
        "\n",
        "        # northeastern AZ (coal plants)\n",
        "        elif lat > 35.0 and lng > -110.5:\n",
        "            emissions = base_emissions * (1.20 + np.random.random() * 0.15)  # 20-35% higher\n",
        "\n",
        "        # southern AZ (more solar penetration)\n",
        "        elif lat < 32.5:\n",
        "            emissions = base_emissions * (0.85 + np.random.random() * 0.15)  # 0-30% lower\n",
        "\n",
        "        else:\n",
        "            emissions = base_emissions * (0.90 + np.random.random() * 0.20)  # ±15% variation\n",
        "\n",
        "        loc[\"carbon_emissions\"] = emissions\n",
        "\n",
        "    return locations\n",
        "\n",
        "# water-use efficiency scores\n",
        "def generate_water_efficiency(locations):\n",
        "    # based on climate factors like temperature, humidity, and technology potential\n",
        "\n",
        "    for loc in locations:\n",
        "        lat, lng = loc[\"lat\"], loc[\"lng\"]\n",
        "\n",
        "        # it seems that higher latitudes and elevations typically have better efficiency due to cooler climate, so i just came up with this\n",
        "        base_efficiency = 3.0 + (lat - AZ_MIN_LAT) / (AZ_MAX_LAT - AZ_MIN_LAT) * 2.0\n",
        "\n",
        "        # hot desert areas (e.g., phoenix, yuma) will have poor efficiency\n",
        "        if ((33.0 <= lat <= 34.0) and (-112.5 <= lng <= -111.5)) or (lng < -113.5 and lat < 33.0):\n",
        "            efficiency = 1.5 + np.random.random() * 1.0  # 1.5-2.5 (poor)\n",
        "\n",
        "        #\\higher elevation areas ( e.g., flagstaff, high country) - good efficiency\n",
        "        elif ((35.0 <= lat <= 36.0) and (-112.5 <= lng <= -111.0)):\n",
        "            efficiency = 4.0 + np.random.random() * 1.0  # 4.0-5.0 (excellent)\n",
        "\n",
        "        # mountain transition zones will have moderate to good efficiency\n",
        "        elif ((34.0 <= lat <= 35.0) and (-112.5 <= lng <= -111.0)):\n",
        "            efficiency = 3.0 + np.random.random() * 1.5  # 3.0-4.5 (good)\n",
        "\n",
        "        else:\n",
        "            efficiency = base_efficiency + (np.random.random() - 0.5) * 1.0\n",
        "        efficiency = max(1, min(5, efficiency))\n",
        "        loc[\"water_efficiency\"] = efficiency\n",
        "\n",
        "    return locations\n",
        "\n",
        "# distance function to calculate distances between locations (in km)\n",
        "# lowkey just took this from my old UROP code, idrk if its correct lol\n",
        "def calc_distance(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth radius in kilometers\n",
        "\n",
        "    # Convert latitude and longitude from degrees to radians\n",
        "    lat1_rad = np.radians(lat1)\n",
        "    lon1_rad = np.radians(lon1)\n",
        "    lat2_rad = np.radians(lat2)\n",
        "    lon2_rad = np.radians(lon2)\n",
        "\n",
        "    # Difference in coordinates\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "\n",
        "    a = np.sin(dlat/2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon/2)**2\n",
        "    c = 2 * np.arcsin(np.sqrt(a))\n",
        "    distance = R * c\n",
        "\n",
        "    return distance\n",
        "\n",
        "# REPLACEMENT - RICHARD, taken from here: https://gis.stackexchange.com/questions/425452/calculate-distance-between-two-lat-lon-alt-points-in-python\n",
        "# from pyproj import Geod\n",
        "# g = Geod(ellps=\"WGS84\")\n",
        "# def calc_distance_alt(lat1, lon1, lat2, lon2):\n",
        "#   _, _, distance = g.inv(lon1, lat1, lon2, lat2)\n",
        "#   return distance\n",
        "\n",
        "# check if a lcation is within arizona's boundaries\n",
        "# this is simplified, w/ real implementation i think we should use a proper shape file\n",
        "# no problem! I should be able to do that in a standard manner, with geopandas. Example: https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoSeries.contains.html#geopandas.GeoSeries.contains - RICHARD\n",
        "def is_in_arizona(lat, lng):\n",
        "  # basically i made it a rectangle\n",
        "    return (AZ_MIN_LAT <= lat <= AZ_MAX_LAT) and (AZ_MIN_LNG <= lng <= AZ_MAX_LNG)\n",
        "\n",
        "# optimization function\n",
        "def optimize_data_center_locations(baseline_centers, potential_locations, min_distance=10):\n",
        "    print(f\"Starting optimization with {len(baseline_centers)} data centers and {len(potential_locations)} potential locations\")\n",
        "    np.random.seed(42)\n",
        "\n",
        "    locations = generate_water_stress(potential_locations)\n",
        "    locations = generate_electricity_prices(locations)\n",
        "    locations = generate_carbon_emissions(locations)\n",
        "    locations = generate_water_efficiency(locations)\n",
        "\n",
        "    location_ids = [loc[\"id\"] for loc in locations]\n",
        "    water_stress = {loc[\"id\"]: loc[\"water_stress\"] for loc in locations}\n",
        "    electricity_price = {loc[\"id\"]: loc[\"electricity_price\"] for loc in locations}\n",
        "    carbon_emissions = {loc[\"id\"]: loc[\"carbon_emissions\"] for loc in locations}\n",
        "    water_efficiency = {loc[\"id\"]: loc[\"water_efficiency\"] for loc in locations}\n",
        "\n",
        "    # coords for distance calculations\n",
        "    location_coords = {loc[\"id\"]: (loc[\"lat\"], loc[\"lng\"]) for loc in locations}\n",
        "\n",
        "    # normalization values for the obj function\n",
        "    max_water_stress = max(water_stress.values())\n",
        "    max_electricity_price = max(electricity_price.values())\n",
        "    max_carbon_emissions = max(carbon_emissions.values())\n",
        "    min_water_efficiency = min(water_efficiency.values())\n",
        "    max_water_efficiency = max(water_efficiency.values())\n",
        "\n",
        "    # objective weights, which can be adjusted based on priorities (kind of chose randomly, so if you want to update at any point, feel free :) )\n",
        "    # sounds great! - RICHARD\n",
        "    # my initial reaction is to set water efficiency weight to zero lol.  but i definitely have some biases/leanings, so we can definitely discuss this. - RICHARD\n",
        "    # another interesting question, kind of taking from my UROP last year, is: at what threshold of objective weights, e.g. on water, do we need before it becomes - RICHARD\n",
        "    # unreasonable to place data centers in hot, desert/dry environments? - RICHARD\n",
        "    # I suppose we can try a few different weightings. (water stress, elec, carbon, water efficiency) = (1, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0), (0, 0, 0, 1), (0.35, 0.25, 0.25, 0.15)\n",
        "    weights = {\n",
        "        \"water_stress\": 0.35,\n",
        "        \"electricity_price\": 0.25,\n",
        "        \"carbon_emissions\": 0.25,\n",
        "        \"water_efficiency\": 0.15\n",
        "    }\n",
        "\n",
        "    model = gp.Model(\"DataCenterOptimization\")\n",
        "\n",
        "    # decision variables: x[d,l] = 1 if data center d is placed at location l\n",
        "    x = {}\n",
        "    for d in range(len(baseline_centers)):\n",
        "        for l in location_ids:\n",
        "            x[d, l] = model.addVar(vtype=GRB.BINARY, name=f\"x_{d}_{l}\")\n",
        "\n",
        "    # minimize weighted combination of metrics\n",
        "    obj = gp.LinExpr()\n",
        "\n",
        "    for d in range(len(baseline_centers)):\n",
        "        for l in location_ids:\n",
        "            # note: for water efficiency, higher is better, so we invert it\n",
        "            # out of curiosity, why not 1 - water_efficiency[l]/max_water_efficiency? But works either way! - RICHARD\n",
        "            normalized_water_efficiency = 1 - (water_efficiency[l] - min_water_efficiency) / (max_water_efficiency - min_water_efficiency)\n",
        "\n",
        "            norm_water_stress = water_stress[l] / max_water_stress\n",
        "            norm_electricity = electricity_price[l] / max_electricity_price\n",
        "            norm_emissions = carbon_emissions[l] / max_carbon_emissions\n",
        "\n",
        "\n",
        "            # I believe that relocating a data center costs around $120,000: https://www.google.com/url?q=https://apposite-tech.com/best-practices-data-center-relocation/&sa=D&source=docs&ust=1744170960046669&usg=AOvVaw1nspqunGp5e_Gn6C7xgrJB. - RICHARD\n",
        "            # Perhaps we could also induce a cost for relocating a data center. So you are more reluctant to actually move a data center unless absolutely necessary. - RICHARD\n",
        "\n",
        "            weighted_sum = (\n",
        "                weights[\"water_stress\"] * norm_water_stress +\n",
        "                weights[\"electricity_price\"] * norm_electricity +\n",
        "                weights[\"carbon_emissions\"] * norm_emissions +\n",
        "                weights[\"water_efficiency\"] * normalized_water_efficiency\n",
        "            )\n",
        "\n",
        "            obj += x[d, l] * weighted_sum\n",
        "\n",
        "    model.setObjective(obj, GRB.MINIMIZE)\n",
        "\n",
        "    # each data center must be placed at exactly one location\n",
        "    for d in range(len(baseline_centers)):\n",
        "        model.addConstr(gp.quicksum(x[d, l] for l in location_ids) == 1, f\"one_loc_per_dc_{d}\")\n",
        "\n",
        "    #each location can have at most one data center\n",
        "    for l in location_ids:\n",
        "        model.addConstr(gp.quicksum(x[d, l] for d in range(len(baseline_centers))) <= 1, f\"one_dc_per_loc_{l}\")\n",
        "\n",
        "    # idk if we need this, but its basically setting a minimum distance between data centers since i dont want the model spitting them out on top of eachy other\n",
        "    # Ooo great point. I feel like this gets at our conversation about why you would ever consider clustering data centers. - RICHARD\n",
        "    # However, I wonder if the water scarcity aspect will naturally cause the model to not cram all the data centers together, if they\n",
        "    # start to pile on too much burden on the local water system. - RICHARD\n",
        "    # Anyhow, I believe the fact that you set the locations in a grid pattern at least gives some minimum spacing. We can keep talking\n",
        "    # about this more if needed! - RICHARD\n",
        "    if min_distance > 0:\n",
        "        location_distances = {}\n",
        "        for l1 in location_ids:\n",
        "            for l2 in location_ids:\n",
        "                if l1 < l2:\n",
        "                    lat1, lng1 = location_coords[l1]\n",
        "                    lat2, lng2 = location_coords[l2]\n",
        "                    dist = calc_distance(lat1, lng1, lat2, lng2)\n",
        "                    location_distances[(l1, l2)] = dist\n",
        "                    location_distances[(l2, l1)] = dist\n",
        "\n",
        "        # so basically, for locations that are too close in lat/lon, prevent placing data centers at both\n",
        "        # I wonder if the below step is contributing to the computational bottleneck. But we don't have to worry about it if it runs\n",
        "        # well on your computer locally :) - RICHARD\n",
        "        for (l1, l2), dist in location_distances.items():\n",
        "            if dist < min_distance:\n",
        "                for d1 in range(len(baseline_centers)):\n",
        "                    for d2 in range(d1 + 1, len(baseline_centers)):\n",
        "                        model.addConstr(x[d1, l1] + x[d2, l2] <= 1, f\"min_dist_{d1}_{l1}_{d2}_{l2}\")\n",
        "\n",
        "    model.optimize()\n",
        "\n",
        "    if model.status == GRB.OPTIMAL:\n",
        "        print(f\"Optimal solution found w/ objective value: {model.objVal}\")\n",
        "\n",
        "        # extract the solution\n",
        "        solution = []\n",
        "        for d in range(len(baseline_centers)):\n",
        "            dc = baseline_centers[d]\n",
        "            for l in location_ids:\n",
        "                if x[d, l].x > 0.5:\n",
        "                    loc_info = next(loc for loc in locations if loc[\"id\"] == l)\n",
        "                    solution.append({\n",
        "                        \"data_center_id\": d,\n",
        "                        \"original_city\": dc[\"city\"],\n",
        "                        \"original_lat\": dc[\"lat\"],\n",
        "                        \"original_lng\": dc[\"lng\"],\n",
        "                        \"new_lat\": loc_info[\"lat\"],\n",
        "                        \"new_lng\": loc_info[\"lng\"],\n",
        "                        \"water_stress\": loc_info[\"water_stress\"],\n",
        "                        \"electricity_price\": loc_info[\"electricity_price\"],\n",
        "                        \"carbon_emissions\": loc_info[\"carbon_emissions\"],\n",
        "                        \"water_efficiency\": loc_info[\"water_efficiency\"],\n",
        "                        \"distance\": calc_distance(\n",
        "                            dc[\"lat\"], dc[\"lng\"],\n",
        "                            loc_info[\"lat\"], loc_info[\"lng\"]\n",
        "                        )\n",
        "                        # feel free to adjujst this list, i was just making sure things seemed okay\n",
        "                    })\n",
        "                    break\n",
        "\n",
        "\n",
        "        # LEFT OFF HERE - RICHARD\n",
        "\n",
        "        baseline_metrics = []\n",
        "        for dc in baseline_centers:\n",
        "            min_dist = float('inf')\n",
        "            closest_loc = None\n",
        "\n",
        "            for loc in locations:\n",
        "                dist = calc_distance(dc[\"lat\"], dc[\"lng\"], loc[\"lat\"], loc[\"lng\"])\n",
        "                if dist < min_dist:\n",
        "                    min_dist = dist\n",
        "                    closest_loc = loc\n",
        "\n",
        "            baseline_metrics.append({\n",
        "                \"city\": dc[\"city\"],\n",
        "                \"lat\": dc[\"lat\"],\n",
        "                \"lng\": dc[\"lng\"],\n",
        "                \"water_stress\": closest_loc[\"water_stress\"],\n",
        "                \"electricity_price\": closest_loc[\"electricity_price\"],\n",
        "                \"carbon_emissions\": closest_loc[\"carbon_emissions\"],\n",
        "                \"water_efficiency\": closest_loc[\"water_efficiency\"]\n",
        "            })\n",
        "\n",
        "        # just to see the average metrics\n",
        "        baseline_avg = {\n",
        "            \"water_stress\": sum(b[\"water_stress\"] for b in baseline_metrics) / len(baseline_metrics),\n",
        "            \"electricity_price\": sum(b[\"electricity_price\"] for b in baseline_metrics) / len(baseline_metrics),\n",
        "            \"carbon_emissions\": sum(b[\"carbon_emissions\"] for b in baseline_metrics) / len(baseline_metrics),\n",
        "            \"water_efficiency\": sum(b[\"water_efficiency\"] for b in baseline_metrics) / len(baseline_metrics)\n",
        "        }\n",
        "\n",
        "        optimized_avg = {\n",
        "            \"water_stress\": sum(s[\"water_stress\"] for s in solution) / len(solution),\n",
        "            \"electricity_price\": sum(s[\"electricity_price\"] for s in solution) / len(solution),\n",
        "            \"carbon_emissions\": sum(s[\"carbon_emissions\"] for s in solution) / len(solution),\n",
        "            \"water_efficiency\": sum(s[\"water_efficiency\"] for s in solution) / len(solution),\n",
        "            \"avg_distance\": sum(s[\"distance\"] for s in solution) / len(solution)\n",
        "        }\n",
        "\n",
        "        # here are the metrics compared to the original ben chmark\n",
        "        print(\"\\nMetrics Comparison:\")\n",
        "        print(f\"Water Stress: {baseline_avg['water_stress']:.2f} → {optimized_avg['water_stress']:.2f} \" +\n",
        "              f\"({((baseline_avg['water_stress'] - optimized_avg['water_stress']) / baseline_avg['water_stress'] * 100):.2f}% improvement)\")\n",
        "\n",
        "        print(f\"Electricity Price: ${baseline_avg['electricity_price']:.4f}/kWh → ${optimized_avg['electricity_price']:.4f}/kWh \" +\n",
        "              f\"({((baseline_avg['electricity_price'] - optimized_avg['electricity_price']) / baseline_avg['electricity_price'] * 100):.2f}% improvement)\")\n",
        "\n",
        "        print(f\"Carbon Emissions: {baseline_avg['carbon_emissions']:.4f} tons CO2/MWh → {optimized_avg['carbon_emissions']:.4f} tons CO2/MWh \" +\n",
        "              f\"({((baseline_avg['carbon_emissions'] - optimized_avg['carbon_emissions']) / baseline_avg['carbon_emissions'] * 100):.2f}% improvement)\")\n",
        "\n",
        "        print(f\"Water Efficiency: {baseline_avg['water_efficiency']:.2f} → {optimized_avg['water_efficiency']:.2f} \" +\n",
        "              f\"({((optimized_avg['water_efficiency'] - baseline_avg['water_efficiency']) / baseline_avg['water_efficiency'] * 100):.2f}% improvement)\")\n",
        "\n",
        "        print(f\"Average Relocation Distance: {optimized_avg['avg_distance']:.2f} km\")\n",
        "\n",
        "        return {\n",
        "            \"objective_value\": model.objVal,\n",
        "            \"solution\": solution,\n",
        "            \"baseline\": baseline_metrics,\n",
        "            \"metrics\": {\n",
        "                \"baseline\": baseline_avg,\n",
        "                \"optimized\": optimized_avg\n",
        "            }\n",
        "        }\n",
        "    else:\n",
        "        print(f\"No optimal solution found. Status: {model.status}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "# \\main execution\n",
        "if __name__ == \"__main__\":\n",
        "  #grid resoljution\n",
        "    resolution = 0.25  # 0.25 degrees = ~28 km\n",
        "\n",
        "    # adjust the spacing based on longitude (since longitude degrees are smaller at higher latitudes)\n",
        "    # again, this is from my old UROP, so could be slightly off\n",
        "    avg_lat = (AZ_MIN_LAT + AZ_MAX_LAT) / 2\n",
        "    lng_spacing = resolution / np.cos(np.radians(avg_lat))\n",
        "\n",
        "    # create baseline data centers (129 total for az)\n",
        "    baseline_centers = create_baseline_centers()\n",
        "    print(f\"Created {len(baseline_centers)} baseline data centers\")\n",
        "\n",
        "    # make a grid of potential locations\n",
        "    potential_locations = create_location_grid(resolution)\n",
        "    print(f\"Created {len(potential_locations)} potential locations\")\n",
        "\n",
        "    # minimum distance between data centers (in km)\n",
        "    # just set it to 0 to disable this constraint\n",
        "    min_distance = 20  # km\n",
        "\n",
        "    results = optimize_data_center_locations(baseline_centers, potential_locations, min_distance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HxFfgSQHAYo",
        "outputId": "b2d76246-a160-4b31-8c6b-88cdf33ef4ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gurobipy\n",
            "  Downloading gurobipy-12.0.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (16 kB)\n",
            "Downloading gurobipy-12.0.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (14.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.4/14.4 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gurobipy\n",
            "Successfully installed gurobipy-12.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gurobipy"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
